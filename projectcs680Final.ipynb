{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/stocknews/upload_DJIA_table.csv\n",
      "/kaggle/input/stocknews/Combined_News_DJIA.csv\n",
      "/kaggle/input/stocknews/RedditNews.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:22: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import time\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>17924.240234</td>\n",
       "      <td>18002.380859</td>\n",
       "      <td>17916.910156</td>\n",
       "      <td>17949.369141</td>\n",
       "      <td>82160000</td>\n",
       "      <td>17949.369141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-06-30</td>\n",
       "      <td>17712.759766</td>\n",
       "      <td>17930.609375</td>\n",
       "      <td>17711.800781</td>\n",
       "      <td>17929.990234</td>\n",
       "      <td>133030000</td>\n",
       "      <td>17929.990234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-06-29</td>\n",
       "      <td>17456.019531</td>\n",
       "      <td>17704.509766</td>\n",
       "      <td>17456.019531</td>\n",
       "      <td>17694.679688</td>\n",
       "      <td>106380000</td>\n",
       "      <td>17694.679688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>17190.509766</td>\n",
       "      <td>17409.720703</td>\n",
       "      <td>17190.509766</td>\n",
       "      <td>17409.720703</td>\n",
       "      <td>112190000</td>\n",
       "      <td>17409.720703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-06-27</td>\n",
       "      <td>17355.210938</td>\n",
       "      <td>17355.210938</td>\n",
       "      <td>17063.080078</td>\n",
       "      <td>17140.240234</td>\n",
       "      <td>138740000</td>\n",
       "      <td>17140.240234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date          Open          High           Low         Close  \\\n",
       "0  2016-07-01  17924.240234  18002.380859  17916.910156  17949.369141   \n",
       "1  2016-06-30  17712.759766  17930.609375  17711.800781  17929.990234   \n",
       "2  2016-06-29  17456.019531  17704.509766  17456.019531  17694.679688   \n",
       "3  2016-06-28  17190.509766  17409.720703  17190.509766  17409.720703   \n",
       "4  2016-06-27  17355.210938  17355.210938  17063.080078  17140.240234   \n",
       "\n",
       "      Volume     Adj Close  \n",
       "0   82160000  17949.369141  \n",
       "1  133030000  17929.990234  \n",
       "2  106380000  17694.679688  \n",
       "3  112190000  17409.720703  \n",
       "4  138740000  17140.240234  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('../input/stocknews/upload_DJIA_table.csv')\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>News</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>A 117-year-old woman in Mexico City finally re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>IMF chief backs Athens as permanent Olympic host</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>The president of France says if Brexit won, so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>British Man Who Must Give Police 24 Hours' Not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>100+ Nobel laureates urge Greenpeace to stop o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date                                               News\n",
       "0  2016-07-01  A 117-year-old woman in Mexico City finally re...\n",
       "1  2016-07-01   IMF chief backs Athens as permanent Olympic host\n",
       "2  2016-07-01  The president of France says if Brexit won, so...\n",
       "3  2016-07-01  British Man Who Must Give Police 24 Hours' Not...\n",
       "4  2016-07-01  100+ Nobel laureates urge Greenpeace to stop o..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv('../input/stocknews/RedditNews.csv')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Label</th>\n",
       "      <th>Top1</th>\n",
       "      <th>Top2</th>\n",
       "      <th>Top3</th>\n",
       "      <th>Top4</th>\n",
       "      <th>Top5</th>\n",
       "      <th>Top6</th>\n",
       "      <th>Top7</th>\n",
       "      <th>Top8</th>\n",
       "      <th>...</th>\n",
       "      <th>Top16</th>\n",
       "      <th>Top17</th>\n",
       "      <th>Top18</th>\n",
       "      <th>Top19</th>\n",
       "      <th>Top20</th>\n",
       "      <th>Top21</th>\n",
       "      <th>Top22</th>\n",
       "      <th>Top23</th>\n",
       "      <th>Top24</th>\n",
       "      <th>Top25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-08-08</td>\n",
       "      <td>0</td>\n",
       "      <td>b\"Georgia 'downs two Russian warplanes' as cou...</td>\n",
       "      <td>b'BREAKING: Musharraf to be impeached.'</td>\n",
       "      <td>b'Russia Today: Columns of troops roll into So...</td>\n",
       "      <td>b'Russian tanks are moving towards the capital...</td>\n",
       "      <td>b\"Afghan children raped with 'impunity,' U.N. ...</td>\n",
       "      <td>b'150 Russian tanks have entered South Ossetia...</td>\n",
       "      <td>b\"Breaking: Georgia invades South Ossetia, Rus...</td>\n",
       "      <td>b\"The 'enemy combatent' trials are nothing but...</td>\n",
       "      <td>...</td>\n",
       "      <td>b'Georgia Invades South Ossetia - if Russia ge...</td>\n",
       "      <td>b'Al-Qaeda Faces Islamist Backlash'</td>\n",
       "      <td>b'Condoleezza Rice: \"The US would not act to p...</td>\n",
       "      <td>b'This is a busy day:  The European Union has ...</td>\n",
       "      <td>b\"Georgia will withdraw 1,000 soldiers from Ir...</td>\n",
       "      <td>b'Why the Pentagon Thinks Attacking Iran is a ...</td>\n",
       "      <td>b'Caucasus in crisis: Georgia invades South Os...</td>\n",
       "      <td>b'Indian shoe manufactory  - And again in a se...</td>\n",
       "      <td>b'Visitors Suffering from Mental Illnesses Ban...</td>\n",
       "      <td>b\"No Help for Mexico's Kidnapping Surge\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-08-11</td>\n",
       "      <td>1</td>\n",
       "      <td>b'Why wont America and Nato help us? If they w...</td>\n",
       "      <td>b'Bush puts foot down on Georgian conflict'</td>\n",
       "      <td>b\"Jewish Georgian minister: Thanks to Israeli ...</td>\n",
       "      <td>b'Georgian army flees in disarray as Russians ...</td>\n",
       "      <td>b\"Olympic opening ceremony fireworks 'faked'\"</td>\n",
       "      <td>b'What were the Mossad with fraudulent New Zea...</td>\n",
       "      <td>b'Russia angered by Israeli military sale to G...</td>\n",
       "      <td>b'An American citizen living in S.Ossetia blam...</td>\n",
       "      <td>...</td>\n",
       "      <td>b'Israel and the US behind the Georgian aggres...</td>\n",
       "      <td>b'\"Do not believe TV, neither Russian nor Geor...</td>\n",
       "      <td>b'Riots are still going on in Montreal (Canada...</td>\n",
       "      <td>b'China to overtake US as largest manufacturer'</td>\n",
       "      <td>b'War in South Ossetia [PICS]'</td>\n",
       "      <td>b'Israeli Physicians Group Condemns State Tort...</td>\n",
       "      <td>b' Russia has just beaten the United States ov...</td>\n",
       "      <td>b'Perhaps *the* question about the Georgia - R...</td>\n",
       "      <td>b'Russia is so much better at war'</td>\n",
       "      <td>b\"So this is what it's come to: trading sex fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-08-12</td>\n",
       "      <td>0</td>\n",
       "      <td>b'Remember that adorable 9-year-old who sang a...</td>\n",
       "      <td>b\"Russia 'ends Georgia operation'\"</td>\n",
       "      <td>b'\"If we had no sexual harassment we would hav...</td>\n",
       "      <td>b\"Al-Qa'eda is losing support in Iraq because ...</td>\n",
       "      <td>b'Ceasefire in Georgia: Putin Outmaneuvers the...</td>\n",
       "      <td>b'Why Microsoft and Intel tried to kill the XO...</td>\n",
       "      <td>b'Stratfor: The Russo-Georgian War and the Bal...</td>\n",
       "      <td>b\"I'm Trying to Get a Sense of This Whole Geor...</td>\n",
       "      <td>...</td>\n",
       "      <td>b'U.S. troops still in Georgia (did you know t...</td>\n",
       "      <td>b'Why Russias response to Georgia was right'</td>\n",
       "      <td>b'Gorbachev accuses U.S. of making a \"serious ...</td>\n",
       "      <td>b'Russia, Georgia, and NATO: Cold War Two'</td>\n",
       "      <td>b'Remember that adorable 62-year-old who led y...</td>\n",
       "      <td>b'War in Georgia: The Israeli connection'</td>\n",
       "      <td>b'All signs point to the US encouraging Georgi...</td>\n",
       "      <td>b'Christopher King argues that the US and NATO...</td>\n",
       "      <td>b'America: The New Mexico?'</td>\n",
       "      <td>b\"BBC NEWS | Asia-Pacific | Extinction 'by man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-08-13</td>\n",
       "      <td>0</td>\n",
       "      <td>b' U.S. refuses Israel weapons to attack Iran:...</td>\n",
       "      <td>b\"When the president ordered to attack Tskhinv...</td>\n",
       "      <td>b' Israel clears troops who killed Reuters cam...</td>\n",
       "      <td>b'Britain\\'s policy of being tough on drugs is...</td>\n",
       "      <td>b'Body of 14 year old found in trunk; Latest (...</td>\n",
       "      <td>b'China has moved 10 *million* quake survivors...</td>\n",
       "      <td>b\"Bush announces Operation Get All Up In Russi...</td>\n",
       "      <td>b'Russian forces sink Georgian ships '</td>\n",
       "      <td>...</td>\n",
       "      <td>b'Elephants extinct by 2020?'</td>\n",
       "      <td>b'US humanitarian missions soon in Georgia - i...</td>\n",
       "      <td>b\"Georgia's DDOS came from US sources\"</td>\n",
       "      <td>b'Russian convoy heads into Georgia, violating...</td>\n",
       "      <td>b'Israeli defence minister: US against strike ...</td>\n",
       "      <td>b'Gorbachev: We Had No Choice'</td>\n",
       "      <td>b'Witness: Russian forces head towards Tbilisi...</td>\n",
       "      <td>b' Quarter of Russians blame U.S. for conflict...</td>\n",
       "      <td>b'Georgian president  says US military will ta...</td>\n",
       "      <td>b'2006: Nobel laureate Aleksander Solzhenitsyn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-08-14</td>\n",
       "      <td>1</td>\n",
       "      <td>b'All the experts admit that we should legalis...</td>\n",
       "      <td>b'War in South Osetia - 89 pictures made by a ...</td>\n",
       "      <td>b'Swedish wrestler Ara Abrahamian throws away ...</td>\n",
       "      <td>b'Russia exaggerated the death toll in South O...</td>\n",
       "      <td>b'Missile That Killed 9 Inside Pakistan May Ha...</td>\n",
       "      <td>b\"Rushdie Condemns Random House's Refusal to P...</td>\n",
       "      <td>b'Poland and US agree to missle defense deal. ...</td>\n",
       "      <td>b'Will the Russians conquer Tblisi? Bet on it,...</td>\n",
       "      <td>...</td>\n",
       "      <td>b'Bank analyst forecast Georgian crisis 2 days...</td>\n",
       "      <td>b\"Georgia confict could set back Russia's US r...</td>\n",
       "      <td>b'War in the Caucasus is as much the product o...</td>\n",
       "      <td>b'\"Non-media\" photos of South Ossetia/Georgia ...</td>\n",
       "      <td>b'Georgian TV reporter shot by Russian sniper ...</td>\n",
       "      <td>b'Saudi Arabia: Mother moves to block child ma...</td>\n",
       "      <td>b'Taliban wages war on humanitarian aid workers'</td>\n",
       "      <td>b'Russia: World  \"can forget about\" Georgia\\'s...</td>\n",
       "      <td>b'Darfur rebels accuse Sudan of mounting major...</td>\n",
       "      <td>b'Philippines : Peace Advocate say Muslims nee...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Label                                               Top1  \\\n",
       "0  2008-08-08      0  b\"Georgia 'downs two Russian warplanes' as cou...   \n",
       "1  2008-08-11      1  b'Why wont America and Nato help us? If they w...   \n",
       "2  2008-08-12      0  b'Remember that adorable 9-year-old who sang a...   \n",
       "3  2008-08-13      0  b' U.S. refuses Israel weapons to attack Iran:...   \n",
       "4  2008-08-14      1  b'All the experts admit that we should legalis...   \n",
       "\n",
       "                                                Top2  \\\n",
       "0            b'BREAKING: Musharraf to be impeached.'   \n",
       "1        b'Bush puts foot down on Georgian conflict'   \n",
       "2                 b\"Russia 'ends Georgia operation'\"   \n",
       "3  b\"When the president ordered to attack Tskhinv...   \n",
       "4  b'War in South Osetia - 89 pictures made by a ...   \n",
       "\n",
       "                                                Top3  \\\n",
       "0  b'Russia Today: Columns of troops roll into So...   \n",
       "1  b\"Jewish Georgian minister: Thanks to Israeli ...   \n",
       "2  b'\"If we had no sexual harassment we would hav...   \n",
       "3  b' Israel clears troops who killed Reuters cam...   \n",
       "4  b'Swedish wrestler Ara Abrahamian throws away ...   \n",
       "\n",
       "                                                Top4  \\\n",
       "0  b'Russian tanks are moving towards the capital...   \n",
       "1  b'Georgian army flees in disarray as Russians ...   \n",
       "2  b\"Al-Qa'eda is losing support in Iraq because ...   \n",
       "3  b'Britain\\'s policy of being tough on drugs is...   \n",
       "4  b'Russia exaggerated the death toll in South O...   \n",
       "\n",
       "                                                Top5  \\\n",
       "0  b\"Afghan children raped with 'impunity,' U.N. ...   \n",
       "1      b\"Olympic opening ceremony fireworks 'faked'\"   \n",
       "2  b'Ceasefire in Georgia: Putin Outmaneuvers the...   \n",
       "3  b'Body of 14 year old found in trunk; Latest (...   \n",
       "4  b'Missile That Killed 9 Inside Pakistan May Ha...   \n",
       "\n",
       "                                                Top6  \\\n",
       "0  b'150 Russian tanks have entered South Ossetia...   \n",
       "1  b'What were the Mossad with fraudulent New Zea...   \n",
       "2  b'Why Microsoft and Intel tried to kill the XO...   \n",
       "3  b'China has moved 10 *million* quake survivors...   \n",
       "4  b\"Rushdie Condemns Random House's Refusal to P...   \n",
       "\n",
       "                                                Top7  \\\n",
       "0  b\"Breaking: Georgia invades South Ossetia, Rus...   \n",
       "1  b'Russia angered by Israeli military sale to G...   \n",
       "2  b'Stratfor: The Russo-Georgian War and the Bal...   \n",
       "3  b\"Bush announces Operation Get All Up In Russi...   \n",
       "4  b'Poland and US agree to missle defense deal. ...   \n",
       "\n",
       "                                                Top8  ...  \\\n",
       "0  b\"The 'enemy combatent' trials are nothing but...  ...   \n",
       "1  b'An American citizen living in S.Ossetia blam...  ...   \n",
       "2  b\"I'm Trying to Get a Sense of This Whole Geor...  ...   \n",
       "3             b'Russian forces sink Georgian ships '  ...   \n",
       "4  b'Will the Russians conquer Tblisi? Bet on it,...  ...   \n",
       "\n",
       "                                               Top16  \\\n",
       "0  b'Georgia Invades South Ossetia - if Russia ge...   \n",
       "1  b'Israel and the US behind the Georgian aggres...   \n",
       "2  b'U.S. troops still in Georgia (did you know t...   \n",
       "3                      b'Elephants extinct by 2020?'   \n",
       "4  b'Bank analyst forecast Georgian crisis 2 days...   \n",
       "\n",
       "                                               Top17  \\\n",
       "0                b'Al-Qaeda Faces Islamist Backlash'   \n",
       "1  b'\"Do not believe TV, neither Russian nor Geor...   \n",
       "2       b'Why Russias response to Georgia was right'   \n",
       "3  b'US humanitarian missions soon in Georgia - i...   \n",
       "4  b\"Georgia confict could set back Russia's US r...   \n",
       "\n",
       "                                               Top18  \\\n",
       "0  b'Condoleezza Rice: \"The US would not act to p...   \n",
       "1  b'Riots are still going on in Montreal (Canada...   \n",
       "2  b'Gorbachev accuses U.S. of making a \"serious ...   \n",
       "3             b\"Georgia's DDOS came from US sources\"   \n",
       "4  b'War in the Caucasus is as much the product o...   \n",
       "\n",
       "                                               Top19  \\\n",
       "0  b'This is a busy day:  The European Union has ...   \n",
       "1    b'China to overtake US as largest manufacturer'   \n",
       "2         b'Russia, Georgia, and NATO: Cold War Two'   \n",
       "3  b'Russian convoy heads into Georgia, violating...   \n",
       "4  b'\"Non-media\" photos of South Ossetia/Georgia ...   \n",
       "\n",
       "                                               Top20  \\\n",
       "0  b\"Georgia will withdraw 1,000 soldiers from Ir...   \n",
       "1                     b'War in South Ossetia [PICS]'   \n",
       "2  b'Remember that adorable 62-year-old who led y...   \n",
       "3  b'Israeli defence minister: US against strike ...   \n",
       "4  b'Georgian TV reporter shot by Russian sniper ...   \n",
       "\n",
       "                                               Top21  \\\n",
       "0  b'Why the Pentagon Thinks Attacking Iran is a ...   \n",
       "1  b'Israeli Physicians Group Condemns State Tort...   \n",
       "2          b'War in Georgia: The Israeli connection'   \n",
       "3                     b'Gorbachev: We Had No Choice'   \n",
       "4  b'Saudi Arabia: Mother moves to block child ma...   \n",
       "\n",
       "                                               Top22  \\\n",
       "0  b'Caucasus in crisis: Georgia invades South Os...   \n",
       "1  b' Russia has just beaten the United States ov...   \n",
       "2  b'All signs point to the US encouraging Georgi...   \n",
       "3  b'Witness: Russian forces head towards Tbilisi...   \n",
       "4   b'Taliban wages war on humanitarian aid workers'   \n",
       "\n",
       "                                               Top23  \\\n",
       "0  b'Indian shoe manufactory  - And again in a se...   \n",
       "1  b'Perhaps *the* question about the Georgia - R...   \n",
       "2  b'Christopher King argues that the US and NATO...   \n",
       "3  b' Quarter of Russians blame U.S. for conflict...   \n",
       "4  b'Russia: World  \"can forget about\" Georgia\\'s...   \n",
       "\n",
       "                                               Top24  \\\n",
       "0  b'Visitors Suffering from Mental Illnesses Ban...   \n",
       "1                 b'Russia is so much better at war'   \n",
       "2                        b'America: The New Mexico?'   \n",
       "3  b'Georgian president  says US military will ta...   \n",
       "4  b'Darfur rebels accuse Sudan of mounting major...   \n",
       "\n",
       "                                               Top25  \n",
       "0           b\"No Help for Mexico's Kidnapping Surge\"  \n",
       "1  b\"So this is what it's come to: trading sex fo...  \n",
       "2  b\"BBC NEWS | Asia-Pacific | Extinction 'by man...  \n",
       "3  b'2006: Nobel laureate Aleksander Solzhenitsyn...  \n",
       "4  b'Philippines : Peace Advocate say Muslims nee...  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imported the file which contains top 25 headlines, stock went up or down(label) and date\n",
    "df = pd.read_csv('../input/stocknews/Combined_News_DJIA.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date     0\n",
       "Label    0\n",
       "Top1     0\n",
       "Top2     0\n",
       "Top3     0\n",
       "Top4     0\n",
       "Top5     0\n",
       "Top6     0\n",
       "Top7     0\n",
       "Top8     0\n",
       "Top9     0\n",
       "Top10    0\n",
       "Top11    0\n",
       "Top12    0\n",
       "Top13    0\n",
       "Top14    0\n",
       "Top15    0\n",
       "Top16    0\n",
       "Top17    0\n",
       "Top18    0\n",
       "Top19    0\n",
       "Top20    0\n",
       "Top21    0\n",
       "Top22    0\n",
       "Top23    1\n",
       "Top24    3\n",
       "Top25    3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are null values in top 23,24,25 headlines. We need to clean the data. We can do it by filling the values with median values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling the null values with median \n",
    "\n",
    "df['Top23'].fillna(df['Top23'].median,inplace=True)\n",
    "df['Top24'].fillna(df['Top24'].median,inplace=True)\n",
    "df['Top25'].fillna(df['Top25'].median,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperating the data into train and test\n",
    "\n",
    "train = df[df['Date'] < '20150101']\n",
    "test = df[df['Date'] > '20141231']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date     0\n",
       "Label    0\n",
       "Top1     0\n",
       "Top2     0\n",
       "Top3     0\n",
       "Top4     0\n",
       "Top5     0\n",
       "Top6     0\n",
       "Top7     0\n",
       "Top8     0\n",
       "Top9     0\n",
       "Top10    0\n",
       "Top11    0\n",
       "Top12    0\n",
       "Top13    0\n",
       "Top14    0\n",
       "Top15    0\n",
       "Top16    0\n",
       "Top17    0\n",
       "Top18    0\n",
       "Top19    0\n",
       "Top20    0\n",
       "Top21    0\n",
       "Top22    0\n",
       "Top23    0\n",
       "Top24    0\n",
       "Top25    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating the data into train and test set. 2008-08-08 to 2014-12-31 as training set. And test set as 2015-01-02 to 206-07-01. This is roughly 80%-20% split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[df['Date'] < '20150101']\n",
    "test = df[df['Date'] > '20141231']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1863, 27)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(378, 27)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Label</th>\n",
       "      <th>Top1</th>\n",
       "      <th>Top2</th>\n",
       "      <th>Top3</th>\n",
       "      <th>Top4</th>\n",
       "      <th>Top5</th>\n",
       "      <th>Top6</th>\n",
       "      <th>Top7</th>\n",
       "      <th>Top8</th>\n",
       "      <th>...</th>\n",
       "      <th>Top16</th>\n",
       "      <th>Top17</th>\n",
       "      <th>Top18</th>\n",
       "      <th>Top19</th>\n",
       "      <th>Top20</th>\n",
       "      <th>Top21</th>\n",
       "      <th>Top22</th>\n",
       "      <th>Top23</th>\n",
       "      <th>Top24</th>\n",
       "      <th>Top25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>b georgia  downs two russian warplanes  as cou...</td>\n",
       "      <td>b breaking  musharraf to be impeached</td>\n",
       "      <td>b russia today  columns of troops roll into so...</td>\n",
       "      <td>b russian tanks are moving towards the capital...</td>\n",
       "      <td>b afghan children raped with  impunity   u n  ...</td>\n",
       "      <td>b     russian tanks have entered south ossetia...</td>\n",
       "      <td>b breaking  georgia invades south ossetia  rus...</td>\n",
       "      <td>b the  enemy combatent  trials are nothing but...</td>\n",
       "      <td>...</td>\n",
       "      <td>b georgia invades south ossetia   if russia ge...</td>\n",
       "      <td>b al qaeda faces islamist backlash</td>\n",
       "      <td>b condoleezza rice   the us would not act to p...</td>\n",
       "      <td>b this is a busy day   the european union has ...</td>\n",
       "      <td>b georgia will withdraw       soldiers from ir...</td>\n",
       "      <td>b why the pentagon thinks attacking iran is a ...</td>\n",
       "      <td>b caucasus in crisis  georgia invades south os...</td>\n",
       "      <td>b indian shoe manufactory    and again in a se...</td>\n",
       "      <td>b visitors suffering from mental illnesses ban...</td>\n",
       "      <td>b no help for mexico s kidnapping surge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>b why wont america and nato help us  if they w...</td>\n",
       "      <td>b bush puts foot down on georgian conflict</td>\n",
       "      <td>b jewish georgian minister  thanks to israeli ...</td>\n",
       "      <td>b georgian army flees in disarray as russians ...</td>\n",
       "      <td>b olympic opening ceremony fireworks  faked</td>\n",
       "      <td>b what were the mossad with fraudulent new zea...</td>\n",
       "      <td>b russia angered by israeli military sale to g...</td>\n",
       "      <td>b an american citizen living in s ossetia blam...</td>\n",
       "      <td>...</td>\n",
       "      <td>b israel and the us behind the georgian aggres...</td>\n",
       "      <td>b  do not believe tv  neither russian nor geor...</td>\n",
       "      <td>b riots are still going on in montreal  canada...</td>\n",
       "      <td>b china to overtake us as largest manufacturer</td>\n",
       "      <td>b war in south ossetia  pics</td>\n",
       "      <td>b israeli physicians group condemns state tort...</td>\n",
       "      <td>b  russia has just beaten the united states ov...</td>\n",
       "      <td>b perhaps  the  question about the georgia   r...</td>\n",
       "      <td>b russia is so much better at war</td>\n",
       "      <td>b so this is what it s come to  trading sex fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>b remember that adorable   year old who sang a...</td>\n",
       "      <td>b russia  ends georgia operation</td>\n",
       "      <td>b  if we had no sexual harassment we would hav...</td>\n",
       "      <td>b al qa eda is losing support in iraq because ...</td>\n",
       "      <td>b ceasefire in georgia  putin outmaneuvers the...</td>\n",
       "      <td>b why microsoft and intel tried to kill the xo...</td>\n",
       "      <td>b stratfor  the russo georgian war and the bal...</td>\n",
       "      <td>b i m trying to get a sense of this whole geor...</td>\n",
       "      <td>...</td>\n",
       "      <td>b u s  troops still in georgia  did you know t...</td>\n",
       "      <td>b why russias response to georgia was right</td>\n",
       "      <td>b gorbachev accuses u s  of making a  serious ...</td>\n",
       "      <td>b russia  georgia  and nato  cold war two</td>\n",
       "      <td>b remember that adorable    year old who led y...</td>\n",
       "      <td>b war in georgia  the israeli connection</td>\n",
       "      <td>b all signs point to the us encouraging georgi...</td>\n",
       "      <td>b christopher king argues that the us and nato...</td>\n",
       "      <td>b america  the new mexico</td>\n",
       "      <td>b bbc news   asia pacific   extinction  by man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>b  u s  refuses israel weapons to attack iran ...</td>\n",
       "      <td>b when the president ordered to attack tskhinv...</td>\n",
       "      <td>b  israel clears troops who killed reuters cam...</td>\n",
       "      <td>b britain  s policy of being tough on drugs is...</td>\n",
       "      <td>b body of    year old found in trunk  latest  ...</td>\n",
       "      <td>b china has moved     million  quake survivors...</td>\n",
       "      <td>b bush announces operation get all up in russi...</td>\n",
       "      <td>b russian forces sink georgian ships</td>\n",
       "      <td>...</td>\n",
       "      <td>b elephants extinct by</td>\n",
       "      <td>b us humanitarian missions soon in georgia   i...</td>\n",
       "      <td>b georgia s ddos came from us sources</td>\n",
       "      <td>b russian convoy heads into georgia  violating...</td>\n",
       "      <td>b israeli defence minister  us against strike ...</td>\n",
       "      <td>b gorbachev  we had no choice</td>\n",
       "      <td>b witness  russian forces head towards tbilisi...</td>\n",
       "      <td>b  quarter of russians blame u s  for conflict...</td>\n",
       "      <td>b georgian president  says us military will ta...</td>\n",
       "      <td>b       nobel laureate aleksander solzhenitsyn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>b all the experts admit that we should legalis...</td>\n",
       "      <td>b war in south osetia      pictures made by a ...</td>\n",
       "      <td>b swedish wrestler ara abrahamian throws away ...</td>\n",
       "      <td>b russia exaggerated the death toll in south o...</td>\n",
       "      <td>b missile that killed   inside pakistan may ha...</td>\n",
       "      <td>b rushdie condemns random house s refusal to p...</td>\n",
       "      <td>b poland and us agree to missle defense deal  ...</td>\n",
       "      <td>b will the russians conquer tblisi  bet on it ...</td>\n",
       "      <td>...</td>\n",
       "      <td>b bank analyst forecast georgian crisis   days...</td>\n",
       "      <td>b georgia confict could set back russia s us r...</td>\n",
       "      <td>b war in the caucasus is as much the product o...</td>\n",
       "      <td>b  non media  photos of south ossetia georgia ...</td>\n",
       "      <td>b georgian tv reporter shot by russian sniper ...</td>\n",
       "      <td>b saudi arabia  mother moves to block child ma...</td>\n",
       "      <td>b taliban wages war on humanitarian aid workers</td>\n",
       "      <td>b russia  world   can forget about  georgia  s...</td>\n",
       "      <td>b darfur rebels accuse sudan of mounting major...</td>\n",
       "      <td>b philippines   peace advocate say muslims nee...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Label                                               Top1  \\\n",
       "0                  0  b georgia  downs two russian warplanes  as cou...   \n",
       "1                  1  b why wont america and nato help us  if they w...   \n",
       "2                  0  b remember that adorable   year old who sang a...   \n",
       "3                  0  b  u s  refuses israel weapons to attack iran ...   \n",
       "4                  1  b all the experts admit that we should legalis...   \n",
       "\n",
       "                                                Top2  \\\n",
       "0            b breaking  musharraf to be impeached     \n",
       "1        b bush puts foot down on georgian conflict    \n",
       "2                 b russia  ends georgia operation     \n",
       "3  b when the president ordered to attack tskhinv...   \n",
       "4  b war in south osetia      pictures made by a ...   \n",
       "\n",
       "                                                Top3  \\\n",
       "0  b russia today  columns of troops roll into so...   \n",
       "1  b jewish georgian minister  thanks to israeli ...   \n",
       "2  b  if we had no sexual harassment we would hav...   \n",
       "3  b  israel clears troops who killed reuters cam...   \n",
       "4  b swedish wrestler ara abrahamian throws away ...   \n",
       "\n",
       "                                                Top4  \\\n",
       "0  b russian tanks are moving towards the capital...   \n",
       "1  b georgian army flees in disarray as russians ...   \n",
       "2  b al qa eda is losing support in iraq because ...   \n",
       "3  b britain  s policy of being tough on drugs is...   \n",
       "4  b russia exaggerated the death toll in south o...   \n",
       "\n",
       "                                                Top5  \\\n",
       "0  b afghan children raped with  impunity   u n  ...   \n",
       "1      b olympic opening ceremony fireworks  faked     \n",
       "2  b ceasefire in georgia  putin outmaneuvers the...   \n",
       "3  b body of    year old found in trunk  latest  ...   \n",
       "4  b missile that killed   inside pakistan may ha...   \n",
       "\n",
       "                                                Top6  \\\n",
       "0  b     russian tanks have entered south ossetia...   \n",
       "1  b what were the mossad with fraudulent new zea...   \n",
       "2  b why microsoft and intel tried to kill the xo...   \n",
       "3  b china has moved     million  quake survivors...   \n",
       "4  b rushdie condemns random house s refusal to p...   \n",
       "\n",
       "                                                Top7  \\\n",
       "0  b breaking  georgia invades south ossetia  rus...   \n",
       "1  b russia angered by israeli military sale to g...   \n",
       "2  b stratfor  the russo georgian war and the bal...   \n",
       "3  b bush announces operation get all up in russi...   \n",
       "4  b poland and us agree to missle defense deal  ...   \n",
       "\n",
       "                                                Top8  ...  \\\n",
       "0  b the  enemy combatent  trials are nothing but...  ...   \n",
       "1  b an american citizen living in s ossetia blam...  ...   \n",
       "2  b i m trying to get a sense of this whole geor...  ...   \n",
       "3             b russian forces sink georgian ships    ...   \n",
       "4  b will the russians conquer tblisi  bet on it ...  ...   \n",
       "\n",
       "                                               Top16  \\\n",
       "0  b georgia invades south ossetia   if russia ge...   \n",
       "1  b israel and the us behind the georgian aggres...   \n",
       "2  b u s  troops still in georgia  did you know t...   \n",
       "3                      b elephants extinct by          \n",
       "4  b bank analyst forecast georgian crisis   days...   \n",
       "\n",
       "                                               Top17  \\\n",
       "0                b al qaeda faces islamist backlash    \n",
       "1  b  do not believe tv  neither russian nor geor...   \n",
       "2       b why russias response to georgia was right    \n",
       "3  b us humanitarian missions soon in georgia   i...   \n",
       "4  b georgia confict could set back russia s us r...   \n",
       "\n",
       "                                               Top18  \\\n",
       "0  b condoleezza rice   the us would not act to p...   \n",
       "1  b riots are still going on in montreal  canada...   \n",
       "2  b gorbachev accuses u s  of making a  serious ...   \n",
       "3             b georgia s ddos came from us sources    \n",
       "4  b war in the caucasus is as much the product o...   \n",
       "\n",
       "                                               Top19  \\\n",
       "0  b this is a busy day   the european union has ...   \n",
       "1    b china to overtake us as largest manufacturer    \n",
       "2         b russia  georgia  and nato  cold war two    \n",
       "3  b russian convoy heads into georgia  violating...   \n",
       "4  b  non media  photos of south ossetia georgia ...   \n",
       "\n",
       "                                               Top20  \\\n",
       "0  b georgia will withdraw       soldiers from ir...   \n",
       "1                     b war in south ossetia  pics     \n",
       "2  b remember that adorable    year old who led y...   \n",
       "3  b israeli defence minister  us against strike ...   \n",
       "4  b georgian tv reporter shot by russian sniper ...   \n",
       "\n",
       "                                               Top21  \\\n",
       "0  b why the pentagon thinks attacking iran is a ...   \n",
       "1  b israeli physicians group condemns state tort...   \n",
       "2          b war in georgia  the israeli connection    \n",
       "3                     b gorbachev  we had no choice    \n",
       "4  b saudi arabia  mother moves to block child ma...   \n",
       "\n",
       "                                               Top22  \\\n",
       "0  b caucasus in crisis  georgia invades south os...   \n",
       "1  b  russia has just beaten the united states ov...   \n",
       "2  b all signs point to the us encouraging georgi...   \n",
       "3  b witness  russian forces head towards tbilisi...   \n",
       "4   b taliban wages war on humanitarian aid workers    \n",
       "\n",
       "                                               Top23  \\\n",
       "0  b indian shoe manufactory    and again in a se...   \n",
       "1  b perhaps  the  question about the georgia   r...   \n",
       "2  b christopher king argues that the us and nato...   \n",
       "3  b  quarter of russians blame u s  for conflict...   \n",
       "4  b russia  world   can forget about  georgia  s...   \n",
       "\n",
       "                                               Top24  \\\n",
       "0  b visitors suffering from mental illnesses ban...   \n",
       "1                 b russia is so much better at war    \n",
       "2                        b america  the new mexico     \n",
       "3  b georgian president  says us military will ta...   \n",
       "4  b darfur rebels accuse sudan of mounting major...   \n",
       "\n",
       "                                               Top25  \n",
       "0           b no help for mexico s kidnapping surge   \n",
       "1  b so this is what it s come to  trading sex fo...  \n",
       "2  b bbc news   asia pacific   extinction  by man...  \n",
       "3  b       nobel laureate aleksander solzhenitsyn...  \n",
       "4  b philippines   peace advocate say muslims nee...  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing punctuations and changing all the letters to lowercase for both train and test\n",
    "\n",
    "all_data = [train,test]\n",
    "\n",
    "for df in all_data:\n",
    "    df.replace(\"[^a-zA-Z]\",\" \",regex=True, inplace=True)\n",
    "    for i in df.columns:\n",
    "        if i=='Date':\n",
    "            continue\n",
    "        if i=='Label':\n",
    "            continue\n",
    "        df[i] = df[i].str.lower()\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b georgia  downs two russian warplanes  as countries move to brink of war  b breaking  musharraf to be impeached   b russia today  columns of troops roll into south ossetia  footage from fighting  youtube   b russian tanks are moving towards the capital of south ossetia  which has reportedly been completely destroyed by georgian artillery fire  b afghan children raped with  impunity   u n  official says   this is sick  a three year old was raped and they do nothing  b     russian tanks have entered south ossetia whilst georgia shoots down two russian jets   b breaking  georgia invades south ossetia  russia warned it would intervene on so s side  b the  enemy combatent  trials are nothing but a sham  salim haman has been sentenced to       years  but will be kept longer anyway just because they feel like it   b georgian troops retreat from s  osettain capital  presumably leaving several hundred people killed   video   b did the u s  prep georgia for war with russia   b rice gives green light for israel to attack iran  says u s  has no veto over israeli military ops  b announcing class action lawsuit on behalf of american public against the fbi  b so   russia and georgia are at war and the nyt s top story is opening ceremonies of the olympics   what a fucking disgrace and yet further proof of the decline of journalism   b china tells bush to stay out of other countries  affairs  b did world war iii start today   b georgia invades south ossetia   if russia gets involved  will nato absorb georgia and unleash a full scale war   b al qaeda faces islamist backlash  b condoleezza rice   the us would not act to prevent an israeli strike on iran   israeli defense minister ehud barak   israel is prepared for uncompromising victory in the case of military hostilities    b this is a busy day   the european union has approved new sanctions against iran in protest at its nuclear programme   b georgia will withdraw       soldiers from iraq to help fight off russian forces in georgia s breakaway region of south ossetia  b why the pentagon thinks attacking iran is a bad idea   us news  amp  world report  b caucasus in crisis  georgia invades south ossetia  b indian shoe manufactory    and again in a series of  you do not like your work    b visitors suffering from mental illnesses banned from olympics  b no help for mexico s kidnapping surge '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combining all the headlines in train data into one and appending them into a list \n",
    "\n",
    "headlines = []\n",
    "for row in range(0,len(train.index)):\n",
    "    headlines.append(' '.join(str(x) for x in train.iloc[row,2:]))\n",
    "headlines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining all the headlines in test data into one and appending them into a list \n",
    "\n",
    "test_transform= []\n",
    "for row in range(0,len(test.index)):\n",
    "    test_transform.append(' '.join(str(x) for x in test.iloc[row,2:27]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.839\n",
      "Precision: 0.799\n",
      "Recall: 0.911\n",
      "F1-Score: 0.852\n",
      "max number of features used : 500\n",
      "ngram_range (1,1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.76      0.82       186\n",
      "           1       0.80      0.91      0.85       192\n",
      "\n",
      "    accuracy                           0.84       378\n",
      "   macro avg       0.85      0.84      0.84       378\n",
      "weighted avg       0.85      0.84      0.84       378\n",
      "\n",
      "confusion matrix : [[142  44]\n",
      " [ 17 175]]\n",
      "--- 6.412210941314697 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.854\n",
      "Precision: 0.825\n",
      "Recall: 0.906\n",
      "F1-Score: 0.864\n",
      "max number of features used : 500\n",
      "ngram_range (2,2)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.80      0.84       186\n",
      "           1       0.82      0.91      0.86       192\n",
      "\n",
      "    accuracy                           0.85       378\n",
      "   macro avg       0.86      0.85      0.85       378\n",
      "weighted avg       0.86      0.85      0.85       378\n",
      "\n",
      "confusion matrix : [[149  37]\n",
      " [ 18 174]]\n",
      "--- 5.920429944992065 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.860\n",
      "Precision: 0.849\n",
      "Recall: 0.880\n",
      "F1-Score: 0.864\n",
      "max number of features used : 500\n",
      "ngram_range (3,3)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.84      0.85       186\n",
      "           1       0.85      0.88      0.86       192\n",
      "\n",
      "    accuracy                           0.86       378\n",
      "   macro avg       0.86      0.86      0.86       378\n",
      "weighted avg       0.86      0.86      0.86       378\n",
      "\n",
      "confusion matrix : [[156  30]\n",
      " [ 23 169]]\n",
      "--- 6.883607625961304 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.802\n",
      "Precision: 0.762\n",
      "Recall: 0.885\n",
      "F1-Score: 0.819\n",
      "max number of features used : 500\n",
      "ngram_range (4,4)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.72      0.78       186\n",
      "           1       0.76      0.89      0.82       192\n",
      "\n",
      "    accuracy                           0.80       378\n",
      "   macro avg       0.81      0.80      0.80       378\n",
      "weighted avg       0.81      0.80      0.80       378\n",
      "\n",
      "confusion matrix : [[133  53]\n",
      " [ 22 170]]\n",
      "--- 8.31070852279663 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.638\n",
      "Precision: 0.597\n",
      "Recall: 0.880\n",
      "F1-Score: 0.712\n",
      "max number of features used : 500\n",
      "ngram_range (5,5)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.39      0.51       186\n",
      "           1       0.60      0.88      0.71       192\n",
      "\n",
      "    accuracy                           0.64       378\n",
      "   macro avg       0.68      0.63      0.61       378\n",
      "weighted avg       0.68      0.64      0.61       378\n",
      "\n",
      "confusion matrix : [[ 72 114]\n",
      " [ 23 169]]\n",
      "--- 7.475788354873657 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.841\n",
      "Precision: 0.803\n",
      "Recall: 0.911\n",
      "F1-Score: 0.854\n",
      "max number of features used : 1000\n",
      "ngram_range (1,1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.77      0.83       186\n",
      "           1       0.80      0.91      0.85       192\n",
      "\n",
      "    accuracy                           0.84       378\n",
      "   macro avg       0.85      0.84      0.84       378\n",
      "weighted avg       0.85      0.84      0.84       378\n",
      "\n",
      "confusion matrix : [[143  43]\n",
      " [ 17 175]]\n",
      "--- 6.228760242462158 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.841\n",
      "Precision: 0.827\n",
      "Recall: 0.870\n",
      "F1-Score: 0.848\n",
      "max number of features used : 1000\n",
      "ngram_range (2,2)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.81      0.83       186\n",
      "           1       0.83      0.87      0.85       192\n",
      "\n",
      "    accuracy                           0.84       378\n",
      "   macro avg       0.84      0.84      0.84       378\n",
      "weighted avg       0.84      0.84      0.84       378\n",
      "\n",
      "confusion matrix : [[151  35]\n",
      " [ 25 167]]\n",
      "--- 6.1160666942596436 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.873\n",
      "Precision: 0.883\n",
      "Recall: 0.865\n",
      "F1-Score: 0.874\n",
      "max number of features used : 1000\n",
      "ngram_range (3,3)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.88      0.87       186\n",
      "           1       0.88      0.86      0.87       192\n",
      "\n",
      "    accuracy                           0.87       378\n",
      "   macro avg       0.87      0.87      0.87       378\n",
      "weighted avg       0.87      0.87      0.87       378\n",
      "\n",
      "confusion matrix : [[164  22]\n",
      " [ 26 166]]\n",
      "--- 6.59783935546875 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.810\n",
      "Precision: 0.797\n",
      "Recall: 0.839\n",
      "F1-Score: 0.817\n",
      "max number of features used : 1000\n",
      "ngram_range (4,4)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.78      0.80       186\n",
      "           1       0.80      0.84      0.82       192\n",
      "\n",
      "    accuracy                           0.81       378\n",
      "   macro avg       0.81      0.81      0.81       378\n",
      "weighted avg       0.81      0.81      0.81       378\n",
      "\n",
      "confusion matrix : [[145  41]\n",
      " [ 31 161]]\n",
      "--- 7.75156044960022 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.720\n",
      "Precision: 0.671\n",
      "Recall: 0.880\n",
      "F1-Score: 0.761\n",
      "max number of features used : 1000\n",
      "ngram_range (5,5)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.55      0.66       186\n",
      "           1       0.67      0.88      0.76       192\n",
      "\n",
      "    accuracy                           0.72       378\n",
      "   macro avg       0.74      0.72      0.71       378\n",
      "weighted avg       0.74      0.72      0.71       378\n",
      "\n",
      "confusion matrix : [[103  83]\n",
      " [ 23 169]]\n",
      "--- 7.186038970947266 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.854\n",
      "Precision: 0.819\n",
      "Recall: 0.917\n",
      "F1-Score: 0.865\n",
      "max number of features used : 3000\n",
      "ngram_range (1,1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.79      0.84       186\n",
      "           1       0.82      0.92      0.86       192\n",
      "\n",
      "    accuracy                           0.85       378\n",
      "   macro avg       0.86      0.85      0.85       378\n",
      "weighted avg       0.86      0.85      0.85       378\n",
      "\n",
      "confusion matrix : [[147  39]\n",
      " [ 16 176]]\n",
      "--- 6.118721961975098 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.833\n",
      "Precision: 0.809\n",
      "Recall: 0.880\n",
      "F1-Score: 0.843\n",
      "max number of features used : 3000\n",
      "ngram_range (2,2)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.78      0.82       186\n",
      "           1       0.81      0.88      0.84       192\n",
      "\n",
      "    accuracy                           0.83       378\n",
      "   macro avg       0.84      0.83      0.83       378\n",
      "weighted avg       0.84      0.83      0.83       378\n",
      "\n",
      "confusion matrix : [[146  40]\n",
      " [ 23 169]]\n",
      "--- 5.997276782989502 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.878\n",
      "Precision: 0.858\n",
      "Recall: 0.911\n",
      "F1-Score: 0.884\n",
      "max number of features used : 3000\n",
      "ngram_range (3,3)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.84      0.87       186\n",
      "           1       0.86      0.91      0.88       192\n",
      "\n",
      "    accuracy                           0.88       378\n",
      "   macro avg       0.88      0.88      0.88       378\n",
      "weighted avg       0.88      0.88      0.88       378\n",
      "\n",
      "confusion matrix : [[157  29]\n",
      " [ 17 175]]\n",
      "--- 8.0526864528656 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.854\n",
      "Precision: 0.816\n",
      "Recall: 0.922\n",
      "F1-Score: 0.866\n",
      "max number of features used : 3000\n",
      "ngram_range (4,4)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.78      0.84       186\n",
      "           1       0.82      0.92      0.87       192\n",
      "\n",
      "    accuracy                           0.85       378\n",
      "   macro avg       0.86      0.85      0.85       378\n",
      "weighted avg       0.86      0.85      0.85       378\n",
      "\n",
      "confusion matrix : [[146  40]\n",
      " [ 15 177]]\n",
      "--- 8.290827989578247 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.796\n",
      "Precision: 0.747\n",
      "Recall: 0.906\n",
      "F1-Score: 0.819\n",
      "max number of features used : 3000\n",
      "ngram_range (5,5)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.68      0.77       186\n",
      "           1       0.75      0.91      0.82       192\n",
      "\n",
      "    accuracy                           0.80       378\n",
      "   macro avg       0.81      0.79      0.79       378\n",
      "weighted avg       0.81      0.80      0.79       378\n",
      "\n",
      "confusion matrix : [[127  59]\n",
      " [ 18 174]]\n",
      "--- 8.171552419662476 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n"
     ]
    }
   ],
   "source": [
    "max_features_num = [500,1000,3000]\n",
    "ngram = [1,2,3,4,5]\n",
    "accRF = []\n",
    "timeRF = []\n",
    "T = 0\n",
    "for i in max_features_num:\n",
    "    for j in ngram:\n",
    "        start_time = time.time()\n",
    "        countvector=CountVectorizer(ngram_range=(j,j),max_features=i)\n",
    "        traindataset=countvector.fit_transform(headlines)\n",
    "        test_dataset = countvector.transform(test_transform)\n",
    "\n",
    "        RF = RandomForestClassifier(n_estimators=200, criterion='entropy')\n",
    "        RF.fit(traindataset,train['Label'])\n",
    "        predictions = RF.predict(test_dataset)\n",
    "        \n",
    "        acc=accuracy_score(test['Label'],predictions)\n",
    "        accRF.append(acc)\n",
    "        print('Accuracy: %.3f' % acc)\n",
    "        \n",
    "        precision = precision_score(test[\"Label\"], predictions, average='binary')\n",
    "        print('Precision: %.3f' % precision)\n",
    "        \n",
    "        recall = recall_score(test[\"Label\"], predictions, average='binary')\n",
    "        print('Recall: %.3f' % recall)\n",
    "        \n",
    "        score = f1_score(test[\"Label\"], predictions, average='binary')\n",
    "        print('F1-Score: %.3f' % score)\n",
    "        \n",
    "        print('max number of features used : {}'.format(i))\n",
    "        print('ngram_range ({},{})'.format(j,j))\n",
    "        print (classification_report(test[\"Label\"], predictions))\n",
    "       \n",
    "        \n",
    "        matrix=confusion_matrix(test['Label'],predictions)\n",
    "        print('confusion matrix : {}'.format(matrix))\n",
    "        \n",
    "        \n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        \n",
    "        T = time.time() - start_time\n",
    "        timeRF.append(T)\n",
    "        print('==========================================================================')\n",
    "        print('==========================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.412256479263306,\n",
       " 5.920471906661987,\n",
       " 6.883655071258545,\n",
       " 8.310741662979126,\n",
       " 7.475931644439697,\n",
       " 6.228800535202026,\n",
       " 6.116100549697876,\n",
       " 6.597879409790039,\n",
       " 7.751594066619873,\n",
       " 7.186203718185425,\n",
       " 6.1187684535980225,\n",
       " 5.997309446334839,\n",
       " 8.0527184009552,\n",
       " 8.290871143341064,\n",
       " 8.171597003936768]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8386243386243386,\n",
       " 0.8544973544973545,\n",
       " 0.8597883597883598,\n",
       " 0.8015873015873016,\n",
       " 0.6375661375661376,\n",
       " 0.8412698412698413,\n",
       " 0.8412698412698413,\n",
       " 0.873015873015873,\n",
       " 0.8095238095238095,\n",
       " 0.7195767195767195,\n",
       " 0.8544973544973545,\n",
       " 0.8333333333333334,\n",
       " 0.8783068783068783,\n",
       " 0.8544973544973545,\n",
       " 0.7962962962962963]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.844\n",
      "Precision: 0.845\n",
      "Recall: 0.849\n",
      "F1-Score: 0.847\n",
      "max number of features used : 500\n",
      "ngram_range (1,1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.84      0.84       186\n",
      "           1       0.84      0.85      0.85       192\n",
      "\n",
      "    accuracy                           0.84       378\n",
      "   macro avg       0.84      0.84      0.84       378\n",
      "weighted avg       0.84      0.84      0.84       378\n",
      "\n",
      "confusion matrix : [[156  30]\n",
      " [ 29 163]]\n",
      "--- 4.167260646820068 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.844\n",
      "Precision: 0.821\n",
      "Recall: 0.885\n",
      "F1-Score: 0.852\n",
      "max number of features used : 500\n",
      "ngram_range (2,2)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.80      0.83       186\n",
      "           1       0.82      0.89      0.85       192\n",
      "\n",
      "    accuracy                           0.84       378\n",
      "   macro avg       0.85      0.84      0.84       378\n",
      "weighted avg       0.85      0.84      0.84       378\n",
      "\n",
      "confusion matrix : [[149  37]\n",
      " [ 22 170]]\n",
      "--- 6.411141633987427 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.754\n",
      "Precision: 0.735\n",
      "Recall: 0.807\n",
      "F1-Score: 0.769\n",
      "max number of features used : 500\n",
      "ngram_range (3,3)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.70      0.74       186\n",
      "           1       0.73      0.81      0.77       192\n",
      "\n",
      "    accuracy                           0.75       378\n",
      "   macro avg       0.76      0.75      0.75       378\n",
      "weighted avg       0.76      0.75      0.75       378\n",
      "\n",
      "confusion matrix : [[130  56]\n",
      " [ 37 155]]\n",
      "--- 7.2217628955841064 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.635\n",
      "Precision: 0.603\n",
      "Recall: 0.823\n",
      "F1-Score: 0.696\n",
      "max number of features used : 500\n",
      "ngram_range (4,4)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.44      0.54       186\n",
      "           1       0.60      0.82      0.70       192\n",
      "\n",
      "    accuracy                           0.63       378\n",
      "   macro avg       0.65      0.63      0.62       378\n",
      "weighted avg       0.65      0.63      0.62       378\n",
      "\n",
      "confusion matrix : [[ 82 104]\n",
      " [ 34 158]]\n",
      "--- 8.304591417312622 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.550\n",
      "Precision: 0.533\n",
      "Recall: 0.922\n",
      "F1-Score: 0.676\n",
      "max number of features used : 500\n",
      "ngram_range (5,5)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.17      0.27       186\n",
      "           1       0.53      0.92      0.68       192\n",
      "\n",
      "    accuracy                           0.55       378\n",
      "   macro avg       0.60      0.54      0.47       378\n",
      "weighted avg       0.60      0.55      0.47       378\n",
      "\n",
      "confusion matrix : [[ 31 155]\n",
      " [ 15 177]]\n",
      "--- 8.028371810913086 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.857\n",
      "Precision: 0.863\n",
      "Recall: 0.854\n",
      "F1-Score: 0.859\n",
      "max number of features used : 1000\n",
      "ngram_range (1,1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.86      0.86       186\n",
      "           1       0.86      0.85      0.86       192\n",
      "\n",
      "    accuracy                           0.86       378\n",
      "   macro avg       0.86      0.86      0.86       378\n",
      "weighted avg       0.86      0.86      0.86       378\n",
      "\n",
      "confusion matrix : [[160  26]\n",
      " [ 28 164]]\n",
      "--- 8.287707090377808 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.852\n",
      "Precision: 0.858\n",
      "Recall: 0.849\n",
      "F1-Score: 0.853\n",
      "max number of features used : 1000\n",
      "ngram_range (2,2)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85       186\n",
      "           1       0.86      0.85      0.85       192\n",
      "\n",
      "    accuracy                           0.85       378\n",
      "   macro avg       0.85      0.85      0.85       378\n",
      "weighted avg       0.85      0.85      0.85       378\n",
      "\n",
      "confusion matrix : [[159  27]\n",
      " [ 29 163]]\n",
      "--- 9.390227317810059 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.749\n",
      "Precision: 0.732\n",
      "Recall: 0.797\n",
      "F1-Score: 0.763\n",
      "max number of features used : 1000\n",
      "ngram_range (3,3)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.70      0.73       186\n",
      "           1       0.73      0.80      0.76       192\n",
      "\n",
      "    accuracy                           0.75       378\n",
      "   macro avg       0.75      0.75      0.75       378\n",
      "weighted avg       0.75      0.75      0.75       378\n",
      "\n",
      "confusion matrix : [[130  56]\n",
      " [ 39 153]]\n",
      "--- 10.298697233200073 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.667\n",
      "Precision: 0.621\n",
      "Recall: 0.880\n",
      "F1-Score: 0.728\n",
      "max number of features used : 1000\n",
      "ngram_range (4,4)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.45      0.57       186\n",
      "           1       0.62      0.88      0.73       192\n",
      "\n",
      "    accuracy                           0.67       378\n",
      "   macro avg       0.70      0.66      0.65       378\n",
      "weighted avg       0.70      0.67      0.65       378\n",
      "\n",
      "confusion matrix : [[ 83 103]\n",
      " [ 23 169]]\n",
      "--- 10.409991025924683 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.550\n",
      "Precision: 0.533\n",
      "Recall: 0.922\n",
      "F1-Score: 0.676\n",
      "max number of features used : 1000\n",
      "ngram_range (5,5)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.17      0.27       186\n",
      "           1       0.53      0.92      0.68       192\n",
      "\n",
      "    accuracy                           0.55       378\n",
      "   macro avg       0.60      0.54      0.47       378\n",
      "weighted avg       0.60      0.55      0.47       378\n",
      "\n",
      "confusion matrix : [[ 31 155]\n",
      " [ 15 177]]\n",
      "--- 10.470800399780273 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.828\n",
      "Precision: 0.829\n",
      "Recall: 0.833\n",
      "F1-Score: 0.831\n",
      "max number of features used : 3000\n",
      "ngram_range (1,1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.82      0.82       186\n",
      "           1       0.83      0.83      0.83       192\n",
      "\n",
      "    accuracy                           0.83       378\n",
      "   macro avg       0.83      0.83      0.83       378\n",
      "weighted avg       0.83      0.83      0.83       378\n",
      "\n",
      "confusion matrix : [[153  33]\n",
      " [ 32 160]]\n",
      "--- 21.23427176475525 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.841\n",
      "Precision: 0.837\n",
      "Recall: 0.854\n",
      "F1-Score: 0.845\n",
      "max number of features used : 3000\n",
      "ngram_range (2,2)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.83      0.84       186\n",
      "           1       0.84      0.85      0.85       192\n",
      "\n",
      "    accuracy                           0.84       378\n",
      "   macro avg       0.84      0.84      0.84       378\n",
      "weighted avg       0.84      0.84      0.84       378\n",
      "\n",
      "confusion matrix : [[154  32]\n",
      " [ 28 164]]\n",
      "--- 21.61635994911194 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.791\n",
      "Precision: 0.781\n",
      "Recall: 0.818\n",
      "F1-Score: 0.799\n",
      "max number of features used : 3000\n",
      "ngram_range (3,3)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.76      0.78       186\n",
      "           1       0.78      0.82      0.80       192\n",
      "\n",
      "    accuracy                           0.79       378\n",
      "   macro avg       0.79      0.79      0.79       378\n",
      "weighted avg       0.79      0.79      0.79       378\n",
      "\n",
      "confusion matrix : [[142  44]\n",
      " [ 35 157]]\n",
      "--- 21.46881890296936 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.667\n",
      "Precision: 0.621\n",
      "Recall: 0.880\n",
      "F1-Score: 0.728\n",
      "max number of features used : 3000\n",
      "ngram_range (4,4)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.45      0.57       186\n",
      "           1       0.62      0.88      0.73       192\n",
      "\n",
      "    accuracy                           0.67       378\n",
      "   macro avg       0.70      0.66      0.65       378\n",
      "weighted avg       0.70      0.67      0.65       378\n",
      "\n",
      "confusion matrix : [[ 83 103]\n",
      " [ 23 169]]\n",
      "--- 23.932562112808228 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.550\n",
      "Precision: 0.533\n",
      "Recall: 0.922\n",
      "F1-Score: 0.676\n",
      "max number of features used : 3000\n",
      "ngram_range (5,5)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.17      0.27       186\n",
      "           1       0.53      0.92      0.68       192\n",
      "\n",
      "    accuracy                           0.55       378\n",
      "   macro avg       0.60      0.54      0.47       378\n",
      "weighted avg       0.60      0.55      0.47       378\n",
      "\n",
      "confusion matrix : [[ 31 155]\n",
      " [ 15 177]]\n",
      "--- 22.55449938774109 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n"
     ]
    }
   ],
   "source": [
    "max_features_num = [500,1000,3000]\n",
    "ngram = [1,2,3,4,5]\n",
    "accXG = []\n",
    "timeXG = []\n",
    "T = 0\n",
    "for i in max_features_num:\n",
    "    for j in ngram:\n",
    "        start_time = time.time()\n",
    "        countvector=CountVectorizer(ngram_range=(j,j),max_features=i)\n",
    "        traindataset=countvector.fit_transform(headlines)\n",
    "        test_dataset = countvector.transform(test_transform)\n",
    "\n",
    "        xgb = XGBClassifier(random_state =1)\n",
    "        xgb.fit(pd.DataFrame(traindataset.todense(), columns=countvector.get_feature_names()),train['Label'])\n",
    "        predictions = xgb.predict(pd.DataFrame(test_dataset.todense(), columns=countvector.get_feature_names()))\n",
    "        \n",
    "        \n",
    "        acc=accuracy_score(test['Label'],predictions)\n",
    "        accXG.append(acc)\n",
    "        print('Accuracy: %.3f' % acc)\n",
    "        \n",
    "        precision = precision_score(test[\"Label\"], predictions, average='binary')\n",
    "        print('Precision: %.3f' % precision)\n",
    "        \n",
    "        recall = recall_score(test[\"Label\"], predictions, average='binary')\n",
    "        print('Recall: %.3f' % recall)\n",
    "        \n",
    "        score = f1_score(test[\"Label\"], predictions, average='binary')\n",
    "        print('F1-Score: %.3f' % score)\n",
    "        \n",
    "        print('max number of features used : {}'.format(i))\n",
    "        print('ngram_range ({},{})'.format(j,j))\n",
    "        print (classification_report(test[\"Label\"], predictions))\n",
    "       \n",
    "        \n",
    "        matrix=confusion_matrix(test['Label'],predictions)\n",
    "        print('confusion matrix : {}'.format(matrix))\n",
    "        \n",
    "        \n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        T = time.time() - start_time\n",
    "        timeXG.append(T)\n",
    "        print('==========================================================================')\n",
    "        print('==========================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.1674113273620605,\n",
       " 6.411201000213623,\n",
       " 7.2219078540802,\n",
       " 8.304632186889648,\n",
       " 8.028504610061646,\n",
       " 8.287952661514282,\n",
       " 9.390459775924683,\n",
       " 10.298734426498413,\n",
       " 10.410127878189087,\n",
       " 10.470978260040283,\n",
       " 21.234405517578125,\n",
       " 21.61648654937744,\n",
       " 21.46885633468628,\n",
       " 23.932716846466064,\n",
       " 22.554662466049194]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeXG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.843915343915344,\n",
       " 0.843915343915344,\n",
       " 0.753968253968254,\n",
       " 0.6349206349206349,\n",
       " 0.5502645502645502,\n",
       " 0.8571428571428571,\n",
       " 0.8518518518518519,\n",
       " 0.7486772486772487,\n",
       " 0.6666666666666666,\n",
       " 0.5502645502645502,\n",
       " 0.828042328042328,\n",
       " 0.8412698412698413,\n",
       " 0.791005291005291,\n",
       " 0.6666666666666666,\n",
       " 0.5502645502645502]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accXG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.646\n",
      "Precision: 0.646\n",
      "Recall: 0.667\n",
      "F1-Score: 0.656\n",
      "max number of features used : 500\n",
      "ngram_range (1,1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.62      0.63       186\n",
      "           1       0.65      0.67      0.66       192\n",
      "\n",
      "    accuracy                           0.65       378\n",
      "   macro avg       0.65      0.65      0.65       378\n",
      "weighted avg       0.65      0.65      0.65       378\n",
      "\n",
      "confusion matrix : [[116  70]\n",
      " [ 64 128]]\n",
      "--- 1.5384352207183838 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.690\n",
      "Precision: 0.683\n",
      "Recall: 0.729\n",
      "F1-Score: 0.705\n",
      "max number of features used : 500\n",
      "ngram_range (2,2)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.65      0.67       186\n",
      "           1       0.68      0.73      0.71       192\n",
      "\n",
      "    accuracy                           0.69       378\n",
      "   macro avg       0.69      0.69      0.69       378\n",
      "weighted avg       0.69      0.69      0.69       378\n",
      "\n",
      "confusion matrix : [[121  65]\n",
      " [ 52 140]]\n",
      "--- 3.43066668510437 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.704\n",
      "Precision: 0.692\n",
      "Recall: 0.750\n",
      "F1-Score: 0.720\n",
      "max number of features used : 500\n",
      "ngram_range (3,3)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.66      0.69       186\n",
      "           1       0.69      0.75      0.72       192\n",
      "\n",
      "    accuracy                           0.70       378\n",
      "   macro avg       0.70      0.70      0.70       378\n",
      "weighted avg       0.70      0.70      0.70       378\n",
      "\n",
      "confusion matrix : [[122  64]\n",
      " [ 48 144]]\n",
      "--- 4.370547533035278 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.675\n",
      "Precision: 0.646\n",
      "Recall: 0.797\n",
      "F1-Score: 0.713\n",
      "max number of features used : 500\n",
      "ngram_range (4,4)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.55      0.62       186\n",
      "           1       0.65      0.80      0.71       192\n",
      "\n",
      "    accuracy                           0.67       378\n",
      "   macro avg       0.68      0.67      0.67       378\n",
      "weighted avg       0.68      0.67      0.67       378\n",
      "\n",
      "confusion matrix : [[102  84]\n",
      " [ 39 153]]\n",
      "--- 5.049640655517578 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.606\n",
      "Precision: 0.574\n",
      "Recall: 0.870\n",
      "F1-Score: 0.692\n",
      "max number of features used : 500\n",
      "ngram_range (5,5)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.33      0.45       186\n",
      "           1       0.57      0.87      0.69       192\n",
      "\n",
      "    accuracy                           0.61       378\n",
      "   macro avg       0.64      0.60      0.57       378\n",
      "weighted avg       0.64      0.61      0.57       378\n",
      "\n",
      "confusion matrix : [[ 62 124]\n",
      " [ 25 167]]\n",
      "--- 5.19467830657959 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.743\n",
      "Precision: 0.749\n",
      "Recall: 0.745\n",
      "F1-Score: 0.747\n",
      "max number of features used : 1000\n",
      "ngram_range (1,1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.74      0.74       186\n",
      "           1       0.75      0.74      0.75       192\n",
      "\n",
      "    accuracy                           0.74       378\n",
      "   macro avg       0.74      0.74      0.74       378\n",
      "weighted avg       0.74      0.74      0.74       378\n",
      "\n",
      "confusion matrix : [[138  48]\n",
      " [ 49 143]]\n",
      "--- 1.6013526916503906 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.770\n",
      "Precision: 0.775\n",
      "Recall: 0.771\n",
      "F1-Score: 0.773\n",
      "max number of features used : 1000\n",
      "ngram_range (2,2)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.77      0.77       186\n",
      "           1       0.77      0.77      0.77       192\n",
      "\n",
      "    accuracy                           0.77       378\n",
      "   macro avg       0.77      0.77      0.77       378\n",
      "weighted avg       0.77      0.77      0.77       378\n",
      "\n",
      "confusion matrix : [[143  43]\n",
      " [ 44 148]]\n",
      "--- 4.620710611343384 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.759\n",
      "Precision: 0.751\n",
      "Recall: 0.786\n",
      "F1-Score: 0.768\n",
      "max number of features used : 1000\n",
      "ngram_range (3,3)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.73      0.75       186\n",
      "           1       0.75      0.79      0.77       192\n",
      "\n",
      "    accuracy                           0.76       378\n",
      "   macro avg       0.76      0.76      0.76       378\n",
      "weighted avg       0.76      0.76      0.76       378\n",
      "\n",
      "confusion matrix : [[136  50]\n",
      " [ 41 151]]\n",
      "--- 4.509566307067871 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.712\n",
      "Precision: 0.693\n",
      "Recall: 0.776\n",
      "F1-Score: 0.732\n",
      "max number of features used : 1000\n",
      "ngram_range (4,4)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.65      0.69       186\n",
      "           1       0.69      0.78      0.73       192\n",
      "\n",
      "    accuracy                           0.71       378\n",
      "   macro avg       0.71      0.71      0.71       378\n",
      "weighted avg       0.71      0.71      0.71       378\n",
      "\n",
      "confusion matrix : [[120  66]\n",
      " [ 43 149]]\n",
      "--- 4.790453672409058 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.688\n",
      "Precision: 0.636\n",
      "Recall: 0.901\n",
      "F1-Score: 0.746\n",
      "max number of features used : 1000\n",
      "ngram_range (5,5)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.47      0.60       186\n",
      "           1       0.64      0.90      0.75       192\n",
      "\n",
      "    accuracy                           0.69       378\n",
      "   macro avg       0.73      0.68      0.67       378\n",
      "weighted avg       0.73      0.69      0.67       378\n",
      "\n",
      "confusion matrix : [[ 87  99]\n",
      " [ 19 173]]\n",
      "--- 5.067951440811157 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.820\n",
      "Precision: 0.820\n",
      "Recall: 0.828\n",
      "F1-Score: 0.824\n",
      "max number of features used : 3000\n",
      "ngram_range (1,1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.81      0.82       186\n",
      "           1       0.82      0.83      0.82       192\n",
      "\n",
      "    accuracy                           0.82       378\n",
      "   macro avg       0.82      0.82      0.82       378\n",
      "weighted avg       0.82      0.82      0.82       378\n",
      "\n",
      "confusion matrix : [[151  35]\n",
      " [ 33 159]]\n",
      "--- 1.6976425647735596 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.841\n",
      "Precision: 0.855\n",
      "Recall: 0.828\n",
      "F1-Score: 0.841\n",
      "max number of features used : 3000\n",
      "ngram_range (2,2)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.85      0.84       186\n",
      "           1       0.85      0.83      0.84       192\n",
      "\n",
      "    accuracy                           0.84       378\n",
      "   macro avg       0.84      0.84      0.84       378\n",
      "weighted avg       0.84      0.84      0.84       378\n",
      "\n",
      "confusion matrix : [[159  27]\n",
      " [ 33 159]]\n",
      "--- 3.3573670387268066 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.865\n",
      "Precision: 0.865\n",
      "Recall: 0.870\n",
      "F1-Score: 0.868\n",
      "max number of features used : 3000\n",
      "ngram_range (3,3)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86       186\n",
      "           1       0.87      0.87      0.87       192\n",
      "\n",
      "    accuracy                           0.87       378\n",
      "   macro avg       0.87      0.87      0.87       378\n",
      "weighted avg       0.87      0.87      0.87       378\n",
      "\n",
      "confusion matrix : [[160  26]\n",
      " [ 25 167]]\n",
      "--- 4.613770961761475 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.825\n",
      "Precision: 0.812\n",
      "Recall: 0.854\n",
      "F1-Score: 0.832\n",
      "max number of features used : 3000\n",
      "ngram_range (4,4)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.80      0.82       186\n",
      "           1       0.81      0.85      0.83       192\n",
      "\n",
      "    accuracy                           0.83       378\n",
      "   macro avg       0.83      0.82      0.83       378\n",
      "weighted avg       0.83      0.83      0.83       378\n",
      "\n",
      "confusion matrix : [[148  38]\n",
      " [ 28 164]]\n",
      "--- 4.8092875480651855 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.775\n",
      "Precision: 0.722\n",
      "Recall: 0.906\n",
      "F1-Score: 0.804\n",
      "max number of features used : 3000\n",
      "ngram_range (5,5)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.64      0.74       186\n",
      "           1       0.72      0.91      0.80       192\n",
      "\n",
      "    accuracy                           0.78       378\n",
      "   macro avg       0.80      0.77      0.77       378\n",
      "weighted avg       0.79      0.78      0.77       378\n",
      "\n",
      "confusion matrix : [[119  67]\n",
      " [ 18 174]]\n",
      "--- 5.601563453674316 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n"
     ]
    }
   ],
   "source": [
    "max_features_num = [500,1000,3000]\n",
    "ngram = [1,2,3,4,5]\n",
    "accLR = []\n",
    "timeLR = []\n",
    "T = 0\n",
    "for i in max_features_num:\n",
    "    for j in ngram:\n",
    "        start_time = time.time()\n",
    "        countvector=CountVectorizer(ngram_range=(j,j),max_features=i)\n",
    "        traindataset=countvector.fit_transform(headlines)\n",
    "        test_dataset = countvector.transform(test_transform)\n",
    "\n",
    "        LR = LogisticRegression()\n",
    "        LR.fit(traindataset,train['Label'])\n",
    "        predictions = LR.predict(test_dataset)\n",
    "        \n",
    "        acc=accuracy_score(test['Label'],predictions)\n",
    "        accLR.append(acc)\n",
    "        print('Accuracy: %.3f' % acc)\n",
    "        \n",
    "        precision = precision_score(test[\"Label\"], predictions, average='binary')\n",
    "        print('Precision: %.3f' % precision)\n",
    "        \n",
    "        recall = recall_score(test[\"Label\"], predictions, average='binary')\n",
    "        print('Recall: %.3f' % recall)\n",
    "        \n",
    "        score = f1_score(test[\"Label\"], predictions, average='binary')\n",
    "        print('F1-Score: %.3f' % score)\n",
    "        \n",
    "        print('max number of features used : {}'.format(i))\n",
    "        print('ngram_range ({},{})'.format(j,j))\n",
    "        print (classification_report(test[\"Label\"], predictions))\n",
    "       \n",
    "        \n",
    "        matrix=confusion_matrix(test['Label'],predictions)\n",
    "        print('confusion matrix : {}'.format(matrix))\n",
    "        \n",
    "        \n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        T = time.time() - start_time\n",
    "        timeLR.append(T)\n",
    "        print('==========================================================================')\n",
    "        print('==========================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.53847074508667,\n",
       " 3.430708408355713,\n",
       " 4.370587348937988,\n",
       " 5.049689531326294,\n",
       " 5.194726467132568,\n",
       " 1.6013848781585693,\n",
       " 4.620759963989258,\n",
       " 4.509601354598999,\n",
       " 4.790530681610107,\n",
       " 5.067986965179443,\n",
       " 1.6976935863494873,\n",
       " 3.3574130535125732,\n",
       " 4.61380934715271,\n",
       " 4.809327125549316,\n",
       " 5.6019287109375]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6455026455026455,\n",
       " 0.6904761904761905,\n",
       " 0.7037037037037037,\n",
       " 0.6746031746031746,\n",
       " 0.6058201058201058,\n",
       " 0.7433862433862434,\n",
       " 0.7698412698412699,\n",
       " 0.7592592592592593,\n",
       " 0.7116402116402116,\n",
       " 0.6878306878306878,\n",
       " 0.8201058201058201,\n",
       " 0.8412698412698413,\n",
       " 0.8650793650793651,\n",
       " 0.8253968253968254,\n",
       " 0.7751322751322751]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.852\n",
      "Precision: 0.774\n",
      "Recall: 1.000\n",
      "F1-Score: 0.873\n",
      "max number of features used : 500\n",
      "ngram_range (1,1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.70      0.82       186\n",
      "           1       0.77      1.00      0.87       192\n",
      "\n",
      "    accuracy                           0.85       378\n",
      "   macro avg       0.89      0.85      0.85       378\n",
      "weighted avg       0.89      0.85      0.85       378\n",
      "\n",
      "confusion matrix : [[130  56]\n",
      " [  0 192]]\n",
      "--- 6.642781019210815 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.857\n",
      "Precision: 0.783\n",
      "Recall: 0.995\n",
      "F1-Score: 0.876\n",
      "max number of features used : 500\n",
      "ngram_range (2,2)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.72      0.83       186\n",
      "           1       0.78      0.99      0.88       192\n",
      "\n",
      "    accuracy                           0.86       378\n",
      "   macro avg       0.89      0.85      0.85       378\n",
      "weighted avg       0.89      0.86      0.85       378\n",
      "\n",
      "confusion matrix : [[133  53]\n",
      " [  1 191]]\n",
      "--- 4.953260660171509 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.820\n",
      "Precision: 0.844\n",
      "Recall: 0.792\n",
      "F1-Score: 0.817\n",
      "max number of features used : 500\n",
      "ngram_range (3,3)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.85      0.82       186\n",
      "           1       0.84      0.79      0.82       192\n",
      "\n",
      "    accuracy                           0.82       378\n",
      "   macro avg       0.82      0.82      0.82       378\n",
      "weighted avg       0.82      0.82      0.82       378\n",
      "\n",
      "confusion matrix : [[158  28]\n",
      " [ 40 152]]\n",
      "--- 4.7844789028167725 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.698\n",
      "Precision: 0.825\n",
      "Recall: 0.516\n",
      "F1-Score: 0.635\n",
      "max number of features used : 500\n",
      "ngram_range (4,4)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.89      0.74       186\n",
      "           1       0.82      0.52      0.63       192\n",
      "\n",
      "    accuracy                           0.70       378\n",
      "   macro avg       0.73      0.70      0.69       378\n",
      "weighted avg       0.73      0.70      0.69       378\n",
      "\n",
      "confusion matrix : [[165  21]\n",
      " [ 93  99]]\n",
      "--- 5.2826008796691895 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.574\n",
      "Precision: 0.548\n",
      "Recall: 0.917\n",
      "F1-Score: 0.686\n",
      "max number of features used : 500\n",
      "ngram_range (5,5)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.22      0.34       186\n",
      "           1       0.55      0.92      0.69       192\n",
      "\n",
      "    accuracy                           0.57       378\n",
      "   macro avg       0.63      0.57      0.51       378\n",
      "weighted avg       0.63      0.57      0.51       378\n",
      "\n",
      "confusion matrix : [[ 41 145]\n",
      " [ 16 176]]\n",
      "--- 5.752222537994385 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.852\n",
      "Precision: 0.774\n",
      "Recall: 1.000\n",
      "F1-Score: 0.873\n",
      "max number of features used : 1000\n",
      "ngram_range (1,1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.70      0.82       186\n",
      "           1       0.77      1.00      0.87       192\n",
      "\n",
      "    accuracy                           0.85       378\n",
      "   macro avg       0.89      0.85      0.85       378\n",
      "weighted avg       0.89      0.85      0.85       378\n",
      "\n",
      "confusion matrix : [[130  56]\n",
      " [  0 192]]\n",
      "--- 8.988113164901733 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.852\n",
      "Precision: 0.774\n",
      "Recall: 1.000\n",
      "F1-Score: 0.873\n",
      "max number of features used : 1000\n",
      "ngram_range (2,2)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.70      0.82       186\n",
      "           1       0.77      1.00      0.87       192\n",
      "\n",
      "    accuracy                           0.85       378\n",
      "   macro avg       0.89      0.85      0.85       378\n",
      "weighted avg       0.89      0.85      0.85       378\n",
      "\n",
      "confusion matrix : [[130  56]\n",
      " [  0 192]]\n",
      "--- 5.600970506668091 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.847\n",
      "Precision: 0.856\n",
      "Recall: 0.839\n",
      "F1-Score: 0.847\n",
      "max number of features used : 1000\n",
      "ngram_range (3,3)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.85      0.85       186\n",
      "           1       0.86      0.84      0.85       192\n",
      "\n",
      "    accuracy                           0.85       378\n",
      "   macro avg       0.85      0.85      0.85       378\n",
      "weighted avg       0.85      0.85      0.85       378\n",
      "\n",
      "confusion matrix : [[159  27]\n",
      " [ 31 161]]\n",
      "--- 4.921993255615234 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.743\n",
      "Precision: 0.760\n",
      "Recall: 0.724\n",
      "F1-Score: 0.741\n",
      "max number of features used : 1000\n",
      "ngram_range (4,4)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.76      0.75       186\n",
      "           1       0.76      0.72      0.74       192\n",
      "\n",
      "    accuracy                           0.74       378\n",
      "   macro avg       0.74      0.74      0.74       378\n",
      "weighted avg       0.74      0.74      0.74       378\n",
      "\n",
      "confusion matrix : [[142  44]\n",
      " [ 53 139]]\n",
      "--- 5.09616756439209 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.624\n",
      "Precision: 0.581\n",
      "Recall: 0.932\n",
      "F1-Score: 0.716\n",
      "max number of features used : 1000\n",
      "ngram_range (5,5)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.31      0.45       186\n",
      "           1       0.58      0.93      0.72       192\n",
      "\n",
      "    accuracy                           0.62       378\n",
      "   macro avg       0.70      0.62      0.58       378\n",
      "weighted avg       0.70      0.62      0.58       378\n",
      "\n",
      "confusion matrix : [[ 57 129]\n",
      " [ 13 179]]\n",
      "--- 5.132031202316284 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.852\n",
      "Precision: 0.774\n",
      "Recall: 1.000\n",
      "F1-Score: 0.873\n",
      "max number of features used : 3000\n",
      "ngram_range (1,1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.70      0.82       186\n",
      "           1       0.77      1.00      0.87       192\n",
      "\n",
      "    accuracy                           0.85       378\n",
      "   macro avg       0.89      0.85      0.85       378\n",
      "weighted avg       0.89      0.85      0.85       378\n",
      "\n",
      "confusion matrix : [[130  56]\n",
      " [  0 192]]\n",
      "--- 11.335175514221191 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.852\n",
      "Precision: 0.774\n",
      "Recall: 1.000\n",
      "F1-Score: 0.873\n",
      "max number of features used : 3000\n",
      "ngram_range (2,2)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.70      0.82       186\n",
      "           1       0.77      1.00      0.87       192\n",
      "\n",
      "    accuracy                           0.85       378\n",
      "   macro avg       0.89      0.85      0.85       378\n",
      "weighted avg       0.89      0.85      0.85       378\n",
      "\n",
      "confusion matrix : [[130  56]\n",
      " [  0 192]]\n",
      "--- 6.935935735702515 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.847\n",
      "Precision: 0.822\n",
      "Recall: 0.891\n",
      "F1-Score: 0.855\n",
      "max number of features used : 3000\n",
      "ngram_range (3,3)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.80      0.84       186\n",
      "           1       0.82      0.89      0.85       192\n",
      "\n",
      "    accuracy                           0.85       378\n",
      "   macro avg       0.85      0.85      0.85       378\n",
      "weighted avg       0.85      0.85      0.85       378\n",
      "\n",
      "confusion matrix : [[149  37]\n",
      " [ 21 171]]\n",
      "--- 5.50486421585083 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.817\n",
      "Precision: 0.860\n",
      "Recall: 0.766\n",
      "F1-Score: 0.810\n",
      "max number of features used : 3000\n",
      "ngram_range (4,4)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.87      0.82       186\n",
      "           1       0.86      0.77      0.81       192\n",
      "\n",
      "    accuracy                           0.82       378\n",
      "   macro avg       0.82      0.82      0.82       378\n",
      "weighted avg       0.82      0.82      0.82       378\n",
      "\n",
      "confusion matrix : [[162  24]\n",
      " [ 45 147]]\n",
      "--- 5.918213367462158 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.762\n",
      "Precision: 0.713\n",
      "Recall: 0.891\n",
      "F1-Score: 0.792\n",
      "max number of features used : 3000\n",
      "ngram_range (5,5)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.63      0.72       186\n",
      "           1       0.71      0.89      0.79       192\n",
      "\n",
      "    accuracy                           0.76       378\n",
      "   macro avg       0.78      0.76      0.76       378\n",
      "weighted avg       0.78      0.76      0.76       378\n",
      "\n",
      "confusion matrix : [[117  69]\n",
      " [ 21 171]]\n",
      "--- 5.44571852684021 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n"
     ]
    }
   ],
   "source": [
    "max_features_num = [500,1000,3000]\n",
    "ngram = [1,2,3,4,5]\n",
    "accSVM1 = []\n",
    "timeSVM1 = []\n",
    "T = 0\n",
    "for i in max_features_num:\n",
    "    for j in ngram:\n",
    "        start_time = time.time()\n",
    "        countvector=CountVectorizer(ngram_range=(j,j),max_features=i)\n",
    "        traindataset=countvector.fit_transform(headlines)\n",
    "        test_dataset = countvector.transform(test_transform)\n",
    "\n",
    "        SVM1 = svm.SVC(C=1, class_weight='balanced',kernel='rbf', gamma=0.100000000000000000000001, tol=1e-10)\n",
    "        SVM1.fit(traindataset,train['Label'])\n",
    "        predictions = SVM1.predict(test_dataset)\n",
    "        \n",
    "        acc=accuracy_score(test['Label'],predictions)\n",
    "        accSVM1.append(acc)\n",
    "        print('Accuracy: %.3f' % acc)\n",
    "        \n",
    "        precision = precision_score(test[\"Label\"], predictions, average='binary')\n",
    "        print('Precision: %.3f' % precision)\n",
    "        \n",
    "        recall = recall_score(test[\"Label\"], predictions, average='binary')\n",
    "        print('Recall: %.3f' % recall)\n",
    "        \n",
    "        score = f1_score(test[\"Label\"], predictions, average='binary')\n",
    "        print('F1-Score: %.3f' % score)\n",
    "        \n",
    "        print('max number of features used : {}'.format(i))\n",
    "        print('ngram_range ({},{})'.format(j,j))\n",
    "        print (classification_report(test[\"Label\"], predictions))\n",
    "       \n",
    "        \n",
    "        matrix=confusion_matrix(test['Label'],predictions)\n",
    "        print('confusion matrix : {}'.format(matrix))\n",
    "        \n",
    "        \n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        T = time.time() - start_time\n",
    "        timeSVM1.append(T)\n",
    "        print('==========================================================================')\n",
    "        print('==========================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.642816543579102,\n",
       " 4.953297853469849,\n",
       " 4.7845189571380615,\n",
       " 5.2826454639434814,\n",
       " 5.752271413803101,\n",
       " 8.988153219223022,\n",
       " 5.601014852523804,\n",
       " 4.9220521450042725,\n",
       " 5.096294164657593,\n",
       " 5.132066249847412,\n",
       " 11.335211753845215,\n",
       " 6.935970067977905,\n",
       " 5.505012273788452,\n",
       " 5.918375730514526,\n",
       " 5.445751190185547]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeSVM1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8518518518518519,\n",
       " 0.8571428571428571,\n",
       " 0.8201058201058201,\n",
       " 0.6984126984126984,\n",
       " 0.5740740740740741,\n",
       " 0.8518518518518519,\n",
       " 0.8518518518518519,\n",
       " 0.8465608465608465,\n",
       " 0.7433862433862434,\n",
       " 0.6243386243386243,\n",
       " 0.8518518518518519,\n",
       " 0.8518518518518519,\n",
       " 0.8465608465608465,\n",
       " 0.8174603174603174,\n",
       " 0.7619047619047619]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accSVM1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.646\n",
      "Precision: 0.661\n",
      "Recall: 0.620\n",
      "F1-Score: 0.640\n",
      "max number of features used : 500\n",
      "ngram_range (1,1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.67      0.65       186\n",
      "           1       0.66      0.62      0.64       192\n",
      "\n",
      "    accuracy                           0.65       378\n",
      "   macro avg       0.65      0.65      0.65       378\n",
      "weighted avg       0.65      0.65      0.65       378\n",
      "\n",
      "confusion matrix : [[125  61]\n",
      " [ 73 119]]\n",
      "--- 2.4330623149871826 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.672\n",
      "Precision: 0.683\n",
      "Recall: 0.661\n",
      "F1-Score: 0.672\n",
      "max number of features used : 500\n",
      "ngram_range (2,2)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.68      0.67       186\n",
      "           1       0.68      0.66      0.67       192\n",
      "\n",
      "    accuracy                           0.67       378\n",
      "   macro avg       0.67      0.67      0.67       378\n",
      "weighted avg       0.67      0.67      0.67       378\n",
      "\n",
      "confusion matrix : [[127  59]\n",
      " [ 65 127]]\n",
      "--- 3.3853111267089844 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.709\n",
      "Precision: 0.718\n",
      "Recall: 0.703\n",
      "F1-Score: 0.711\n",
      "max number of features used : 500\n",
      "ngram_range (3,3)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.72      0.71       186\n",
      "           1       0.72      0.70      0.71       192\n",
      "\n",
      "    accuracy                           0.71       378\n",
      "   macro avg       0.71      0.71      0.71       378\n",
      "weighted avg       0.71      0.71      0.71       378\n",
      "\n",
      "confusion matrix : [[133  53]\n",
      " [ 57 135]]\n",
      "--- 4.261311292648315 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.693\n",
      "Precision: 0.757\n",
      "Recall: 0.583\n",
      "F1-Score: 0.659\n",
      "max number of features used : 500\n",
      "ngram_range (4,4)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.81      0.72       186\n",
      "           1       0.76      0.58      0.66       192\n",
      "\n",
      "    accuracy                           0.69       378\n",
      "   macro avg       0.70      0.69      0.69       378\n",
      "weighted avg       0.71      0.69      0.69       378\n",
      "\n",
      "confusion matrix : [[150  36]\n",
      " [ 80 112]]\n",
      "--- 5.446556806564331 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.611\n",
      "Precision: 0.580\n",
      "Recall: 0.849\n",
      "F1-Score: 0.689\n",
      "max number of features used : 500\n",
      "ngram_range (5,5)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.37      0.48       186\n",
      "           1       0.58      0.85      0.69       192\n",
      "\n",
      "    accuracy                           0.61       378\n",
      "   macro avg       0.64      0.61      0.58       378\n",
      "weighted avg       0.64      0.61      0.59       378\n",
      "\n",
      "confusion matrix : [[ 68 118]\n",
      " [ 29 163]]\n",
      "--- 4.964950323104858 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.725\n",
      "Precision: 0.744\n",
      "Recall: 0.698\n",
      "F1-Score: 0.720\n",
      "max number of features used : 1000\n",
      "ngram_range (1,1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.75      0.73       186\n",
      "           1       0.74      0.70      0.72       192\n",
      "\n",
      "    accuracy                           0.72       378\n",
      "   macro avg       0.73      0.73      0.72       378\n",
      "weighted avg       0.73      0.72      0.72       378\n",
      "\n",
      "confusion matrix : [[140  46]\n",
      " [ 58 134]]\n",
      "--- 2.5223395824432373 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.759\n",
      "Precision: 0.770\n",
      "Recall: 0.750\n",
      "F1-Score: 0.760\n",
      "max number of features used : 1000\n",
      "ngram_range (2,2)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.77      0.76       186\n",
      "           1       0.77      0.75      0.76       192\n",
      "\n",
      "    accuracy                           0.76       378\n",
      "   macro avg       0.76      0.76      0.76       378\n",
      "weighted avg       0.76      0.76      0.76       378\n",
      "\n",
      "confusion matrix : [[143  43]\n",
      " [ 48 144]]\n",
      "--- 3.460822105407715 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.738\n",
      "Precision: 0.749\n",
      "Recall: 0.729\n",
      "F1-Score: 0.739\n",
      "max number of features used : 1000\n",
      "ngram_range (3,3)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.75      0.74       186\n",
      "           1       0.75      0.73      0.74       192\n",
      "\n",
      "    accuracy                           0.74       378\n",
      "   macro avg       0.74      0.74      0.74       378\n",
      "weighted avg       0.74      0.74      0.74       378\n",
      "\n",
      "confusion matrix : [[139  47]\n",
      " [ 52 140]]\n",
      "--- 4.515836000442505 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.741\n",
      "Precision: 0.753\n",
      "Recall: 0.729\n",
      "F1-Score: 0.741\n",
      "max number of features used : 1000\n",
      "ngram_range (4,4)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.75      0.74       186\n",
      "           1       0.75      0.73      0.74       192\n",
      "\n",
      "    accuracy                           0.74       378\n",
      "   macro avg       0.74      0.74      0.74       378\n",
      "weighted avg       0.74      0.74      0.74       378\n",
      "\n",
      "confusion matrix : [[140  46]\n",
      " [ 52 140]]\n",
      "--- 4.7119996547698975 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.669\n",
      "Precision: 0.633\n",
      "Recall: 0.828\n",
      "F1-Score: 0.718\n",
      "max number of features used : 1000\n",
      "ngram_range (5,5)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.51      0.60       186\n",
      "           1       0.63      0.83      0.72       192\n",
      "\n",
      "    accuracy                           0.67       378\n",
      "   macro avg       0.69      0.67      0.66       378\n",
      "weighted avg       0.69      0.67      0.66       378\n",
      "\n",
      "confusion matrix : [[ 94  92]\n",
      " [ 33 159]]\n",
      "--- 5.141336917877197 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.817\n",
      "Precision: 0.819\n",
      "Recall: 0.823\n",
      "F1-Score: 0.821\n",
      "max number of features used : 3000\n",
      "ngram_range (1,1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.81      0.81       186\n",
      "           1       0.82      0.82      0.82       192\n",
      "\n",
      "    accuracy                           0.82       378\n",
      "   macro avg       0.82      0.82      0.82       378\n",
      "weighted avg       0.82      0.82      0.82       378\n",
      "\n",
      "confusion matrix : [[151  35]\n",
      " [ 34 158]]\n",
      "--- 2.932300329208374 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.849\n",
      "Precision: 0.861\n",
      "Recall: 0.839\n",
      "F1-Score: 0.850\n",
      "max number of features used : 3000\n",
      "ngram_range (2,2)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.86      0.85       186\n",
      "           1       0.86      0.84      0.85       192\n",
      "\n",
      "    accuracy                           0.85       378\n",
      "   macro avg       0.85      0.85      0.85       378\n",
      "weighted avg       0.85      0.85      0.85       378\n",
      "\n",
      "confusion matrix : [[160  26]\n",
      " [ 31 161]]\n",
      "--- 3.4146902561187744 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.857\n",
      "Precision: 0.863\n",
      "Recall: 0.854\n",
      "F1-Score: 0.859\n",
      "max number of features used : 3000\n",
      "ngram_range (3,3)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.86      0.86       186\n",
      "           1       0.86      0.85      0.86       192\n",
      "\n",
      "    accuracy                           0.86       378\n",
      "   macro avg       0.86      0.86      0.86       378\n",
      "weighted avg       0.86      0.86      0.86       378\n",
      "\n",
      "confusion matrix : [[160  26]\n",
      " [ 28 164]]\n",
      "--- 4.996545314788818 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.810\n",
      "Precision: 0.816\n",
      "Recall: 0.807\n",
      "F1-Score: 0.812\n",
      "max number of features used : 3000\n",
      "ngram_range (4,4)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.81      0.81       186\n",
      "           1       0.82      0.81      0.81       192\n",
      "\n",
      "    accuracy                           0.81       378\n",
      "   macro avg       0.81      0.81      0.81       378\n",
      "weighted avg       0.81      0.81      0.81       378\n",
      "\n",
      "confusion matrix : [[151  35]\n",
      " [ 37 155]]\n",
      "--- 5.1259846687316895 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.775\n",
      "Precision: 0.734\n",
      "Recall: 0.875\n",
      "F1-Score: 0.798\n",
      "max number of features used : 3000\n",
      "ngram_range (5,5)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.67      0.75       186\n",
      "           1       0.73      0.88      0.80       192\n",
      "\n",
      "    accuracy                           0.78       378\n",
      "   macro avg       0.79      0.77      0.77       378\n",
      "weighted avg       0.79      0.78      0.77       378\n",
      "\n",
      "confusion matrix : [[125  61]\n",
      " [ 24 168]]\n",
      "--- 5.277683734893799 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n"
     ]
    }
   ],
   "source": [
    "max_features_num = [500,1000,3000]\n",
    "ngram = [1,2,3,4,5]\n",
    "accSVM2=[]\n",
    "timeSVM2 = []\n",
    "T = 0\n",
    "for i in max_features_num:\n",
    "    for j in ngram:\n",
    "        start_time = time.time()\n",
    "        countvector=CountVectorizer(ngram_range=(j,j),max_features=i)\n",
    "        traindataset=countvector.fit_transform(headlines)\n",
    "        test_dataset = countvector.transform(test_transform)\n",
    "\n",
    "        SVM2 = svm.LinearSVC(C=0.1, class_weight='balanced')\n",
    "        SVM2.fit(traindataset,train['Label'])\n",
    "        predictions = SVM2.predict(test_dataset)\n",
    "        \n",
    "        \n",
    "        acc=accuracy_score(test['Label'],predictions)\n",
    "        accSVM2.append(acc)\n",
    "        print('Accuracy: %.3f' % acc)\n",
    "        \n",
    "        precision = precision_score(test[\"Label\"], predictions, average='binary')\n",
    "        print('Precision: %.3f' % precision)\n",
    "        \n",
    "        recall = recall_score(test[\"Label\"], predictions, average='binary')\n",
    "        print('Recall: %.3f' % recall)\n",
    "        \n",
    "        score = f1_score(test[\"Label\"], predictions, average='binary')\n",
    "        print('F1-Score: %.3f' % score)\n",
    "        \n",
    "        print('max number of features used : {}'.format(i))\n",
    "        print('ngram_range ({},{})'.format(j,j))\n",
    "        print (classification_report(test[\"Label\"], predictions))\n",
    "       \n",
    "        \n",
    "        matrix=confusion_matrix(test['Label'],predictions)\n",
    "        print('confusion matrix : {}'.format(matrix))\n",
    "        \n",
    "        \n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        T = time.time() - start_time\n",
    "        timeSVM2.append(T)\n",
    "        print('==========================================================================')\n",
    "        print('==========================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.4331042766571045,\n",
       " 3.385342836380005,\n",
       " 4.261343717575073,\n",
       " 5.446606874465942,\n",
       " 4.9650139808654785,\n",
       " 2.5223793983459473,\n",
       " 3.46085786819458,\n",
       " 4.515882968902588,\n",
       " 4.712031602859497,\n",
       " 5.141366720199585,\n",
       " 2.932464599609375,\n",
       " 3.4147229194641113,\n",
       " 4.996578216552734,\n",
       " 5.126016616821289,\n",
       " 5.277709007263184]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeSVM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6455026455026455,\n",
       " 0.671957671957672,\n",
       " 0.708994708994709,\n",
       " 0.6931216931216931,\n",
       " 0.6111111111111112,\n",
       " 0.7248677248677249,\n",
       " 0.7592592592592593,\n",
       " 0.7380952380952381,\n",
       " 0.7407407407407407,\n",
       " 0.6693121693121693,\n",
       " 0.8174603174603174,\n",
       " 0.8492063492063492,\n",
       " 0.8571428571428571,\n",
       " 0.8095238095238095,\n",
       " 0.7751322751322751]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accSVM2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.767\n",
      "Precision: 0.722\n",
      "Recall: 0.880\n",
      "F1-Score: 0.793\n",
      "max number of features used : 500\n",
      "ngram_range (1,1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.65      0.73       186\n",
      "           1       0.72      0.88      0.79       192\n",
      "\n",
      "    accuracy                           0.77       378\n",
      "   macro avg       0.78      0.77      0.76       378\n",
      "weighted avg       0.78      0.77      0.76       378\n",
      "\n",
      "confusion matrix : [[121  65]\n",
      " [ 23 169]]\n",
      "--- 3.970515489578247 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.714\n",
      "Precision: 0.665\n",
      "Recall: 0.880\n",
      "F1-Score: 0.758\n",
      "max number of features used : 500\n",
      "ngram_range (2,2)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.54      0.65       186\n",
      "           1       0.67      0.88      0.76       192\n",
      "\n",
      "    accuracy                           0.71       378\n",
      "   macro avg       0.74      0.71      0.70       378\n",
      "weighted avg       0.74      0.71      0.71       378\n",
      "\n",
      "confusion matrix : [[101  85]\n",
      " [ 23 169]]\n",
      "--- 4.1517417430877686 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.680\n",
      "Precision: 0.633\n",
      "Recall: 0.880\n",
      "F1-Score: 0.736\n",
      "max number of features used : 500\n",
      "ngram_range (3,3)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.47      0.59       186\n",
      "           1       0.63      0.88      0.74       192\n",
      "\n",
      "    accuracy                           0.68       378\n",
      "   macro avg       0.71      0.68      0.66       378\n",
      "weighted avg       0.71      0.68      0.67       378\n",
      "\n",
      "confusion matrix : [[ 88  98]\n",
      " [ 23 169]]\n",
      "--- 4.588439702987671 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.614\n",
      "Precision: 0.574\n",
      "Recall: 0.932\n",
      "F1-Score: 0.710\n",
      "max number of features used : 500\n",
      "ngram_range (4,4)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.28      0.42       186\n",
      "           1       0.57      0.93      0.71       192\n",
      "\n",
      "    accuracy                           0.61       378\n",
      "   macro avg       0.69      0.61      0.57       378\n",
      "weighted avg       0.69      0.61      0.57       378\n",
      "\n",
      "confusion matrix : [[ 53 133]\n",
      " [ 13 179]]\n",
      "--- 4.886246204376221 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.566\n",
      "Precision: 0.540\n",
      "Recall: 0.974\n",
      "F1-Score: 0.695\n",
      "max number of features used : 500\n",
      "ngram_range (5,5)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.15      0.25       186\n",
      "           1       0.54      0.97      0.70       192\n",
      "\n",
      "    accuracy                           0.57       378\n",
      "   macro avg       0.69      0.56      0.47       378\n",
      "weighted avg       0.69      0.57      0.47       378\n",
      "\n",
      "confusion matrix : [[ 27 159]\n",
      " [  5 187]]\n",
      "--- 5.145906209945679 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.741\n",
      "Precision: 0.712\n",
      "Recall: 0.823\n",
      "F1-Score: 0.763\n",
      "max number of features used : 1000\n",
      "ngram_range (1,1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.66      0.71       186\n",
      "           1       0.71      0.82      0.76       192\n",
      "\n",
      "    accuracy                           0.74       378\n",
      "   macro avg       0.75      0.74      0.74       378\n",
      "weighted avg       0.75      0.74      0.74       378\n",
      "\n",
      "confusion matrix : [[122  64]\n",
      " [ 34 158]]\n",
      "--- 4.59521222114563 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.735\n",
      "Precision: 0.687\n",
      "Recall: 0.880\n",
      "F1-Score: 0.772\n",
      "max number of features used : 1000\n",
      "ngram_range (2,2)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.59      0.69       186\n",
      "           1       0.69      0.88      0.77       192\n",
      "\n",
      "    accuracy                           0.74       378\n",
      "   macro avg       0.76      0.73      0.73       378\n",
      "weighted avg       0.76      0.74      0.73       378\n",
      "\n",
      "confusion matrix : [[109  77]\n",
      " [ 23 169]]\n",
      "--- 4.550476551055908 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.690\n",
      "Precision: 0.640\n",
      "Recall: 0.891\n",
      "F1-Score: 0.745\n",
      "max number of features used : 1000\n",
      "ngram_range (3,3)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.48      0.61       186\n",
      "           1       0.64      0.89      0.75       192\n",
      "\n",
      "    accuracy                           0.69       378\n",
      "   macro avg       0.73      0.69      0.68       378\n",
      "weighted avg       0.72      0.69      0.68       378\n",
      "\n",
      "confusion matrix : [[ 90  96]\n",
      " [ 21 171]]\n",
      "--- 4.63329291343689 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.635\n",
      "Precision: 0.588\n",
      "Recall: 0.943\n",
      "F1-Score: 0.724\n",
      "max number of features used : 1000\n",
      "ngram_range (4,4)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.32      0.46       186\n",
      "           1       0.59      0.94      0.72       192\n",
      "\n",
      "    accuracy                           0.63       378\n",
      "   macro avg       0.72      0.63      0.59       378\n",
      "weighted avg       0.71      0.63      0.59       378\n",
      "\n",
      "confusion matrix : [[ 59 127]\n",
      " [ 11 181]]\n",
      "--- 5.131789207458496 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.595\n",
      "Precision: 0.559\n",
      "Recall: 0.964\n",
      "F1-Score: 0.707\n",
      "max number of features used : 1000\n",
      "ngram_range (5,5)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.22      0.34       186\n",
      "           1       0.56      0.96      0.71       192\n",
      "\n",
      "    accuracy                           0.60       378\n",
      "   macro avg       0.70      0.59      0.53       378\n",
      "weighted avg       0.70      0.60      0.53       378\n",
      "\n",
      "confusion matrix : [[ 40 146]\n",
      " [  7 185]]\n",
      "--- 4.884953260421753 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.780\n",
      "Precision: 0.756\n",
      "Recall: 0.839\n",
      "F1-Score: 0.795\n",
      "max number of features used : 3000\n",
      "ngram_range (1,1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.72      0.76       186\n",
      "           1       0.76      0.84      0.80       192\n",
      "\n",
      "    accuracy                           0.78       378\n",
      "   macro avg       0.78      0.78      0.78       378\n",
      "weighted avg       0.78      0.78      0.78       378\n",
      "\n",
      "confusion matrix : [[134  52]\n",
      " [ 31 161]]\n",
      "--- 6.593947649002075 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.796\n",
      "Precision: 0.753\n",
      "Recall: 0.891\n",
      "F1-Score: 0.816\n",
      "max number of features used : 3000\n",
      "ngram_range (2,2)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.70      0.77       186\n",
      "           1       0.75      0.89      0.82       192\n",
      "\n",
      "    accuracy                           0.80       378\n",
      "   macro avg       0.81      0.79      0.79       378\n",
      "weighted avg       0.81      0.80      0.79       378\n",
      "\n",
      "confusion matrix : [[130  56]\n",
      " [ 21 171]]\n",
      "--- 5.278361797332764 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.733\n",
      "Precision: 0.680\n",
      "Recall: 0.896\n",
      "F1-Score: 0.773\n",
      "max number of features used : 3000\n",
      "ngram_range (3,3)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.56      0.68       186\n",
      "           1       0.68      0.90      0.77       192\n",
      "\n",
      "    accuracy                           0.73       378\n",
      "   macro avg       0.76      0.73      0.72       378\n",
      "weighted avg       0.76      0.73      0.72       378\n",
      "\n",
      "confusion matrix : [[105  81]\n",
      " [ 20 172]]\n",
      "--- 5.424164533615112 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.685\n",
      "Precision: 0.627\n",
      "Recall: 0.938\n",
      "F1-Score: 0.752\n",
      "max number of features used : 3000\n",
      "ngram_range (4,4)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.42      0.57       186\n",
      "           1       0.63      0.94      0.75       192\n",
      "\n",
      "    accuracy                           0.69       378\n",
      "   macro avg       0.75      0.68      0.66       378\n",
      "weighted avg       0.75      0.69      0.66       378\n",
      "\n",
      "confusion matrix : [[ 79 107]\n",
      " [ 12 180]]\n",
      "--- 5.764068841934204 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Accuracy: 0.677\n",
      "Precision: 0.616\n",
      "Recall: 0.969\n",
      "F1-Score: 0.753\n",
      "max number of features used : 3000\n",
      "ngram_range (5,5)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.38      0.53       186\n",
      "           1       0.62      0.97      0.75       192\n",
      "\n",
      "    accuracy                           0.68       378\n",
      "   macro avg       0.77      0.67      0.64       378\n",
      "weighted avg       0.77      0.68      0.65       378\n",
      "\n",
      "confusion matrix : [[ 70 116]\n",
      " [  6 186]]\n",
      "--- 5.283072233200073 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n"
     ]
    }
   ],
   "source": [
    "max_features_num = [500,1000,3000]\n",
    "ngram = [1,2,3,4,5]\n",
    "accGB = []\n",
    "timeGB = []\n",
    "T = 0\n",
    "for i in max_features_num:\n",
    "    for j in ngram:\n",
    "        start_time = time.time()\n",
    "        countvector=CountVectorizer(ngram_range=(j,j),max_features=i)\n",
    "        traindataset=countvector.fit_transform(headlines)\n",
    "        test_dataset = countvector.transform(test_transform)\n",
    "\n",
    "        GB = GradientBoostingClassifier(random_state=0)\n",
    "        GB.fit(traindataset,train['Label'])\n",
    "        predictions = GB.predict(test_dataset)\n",
    "        \n",
    "        \n",
    "        acc=accuracy_score(test['Label'],predictions)\n",
    "        accGB.append(acc)\n",
    "        print('Accuracy: %.3f' % acc)\n",
    "        \n",
    "        precision = precision_score(test[\"Label\"], predictions, average='binary')\n",
    "        print('Precision: %.3f' % precision)\n",
    "        \n",
    "        recall = recall_score(test[\"Label\"], predictions, average='binary')\n",
    "        print('Recall: %.3f' % recall)\n",
    "        \n",
    "        score = f1_score(test[\"Label\"], predictions, average='binary')\n",
    "        print('F1-Score: %.3f' % score)\n",
    "        \n",
    "        print('max number of features used : {}'.format(i))\n",
    "        print('ngram_range ({},{})'.format(j,j))\n",
    "        print (classification_report(test[\"Label\"], predictions))\n",
    "       \n",
    "        \n",
    "        matrix=confusion_matrix(test['Label'],predictions)\n",
    "        print('confusion matrix : {}'.format(matrix))\n",
    "        \n",
    "        \n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        T = time.time() - start_time\n",
    "        timeGB.append(T)\n",
    "        print('==========================================================================')\n",
    "        print('==========================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.970543622970581,\n",
       " 4.151768922805786,\n",
       " 4.588473081588745,\n",
       " 4.886287689208984,\n",
       " 5.1459386348724365,\n",
       " 4.5953590869903564,\n",
       " 4.550513029098511,\n",
       " 4.633326530456543,\n",
       " 5.131822347640991,\n",
       " 4.884997129440308,\n",
       " 6.593981504440308,\n",
       " 5.278387546539307,\n",
       " 5.4241979122161865,\n",
       " 5.764100790023804,\n",
       " 5.283147573471069]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7671957671957672,\n",
       " 0.7142857142857143,\n",
       " 0.6798941798941799,\n",
       " 0.6137566137566137,\n",
       " 0.5661375661375662,\n",
       " 0.7407407407407407,\n",
       " 0.7354497354497355,\n",
       " 0.6904761904761905,\n",
       " 0.6349206349206349,\n",
       " 0.5952380952380952,\n",
       " 0.7804232804232805,\n",
       " 0.7962962962962963,\n",
       " 0.7328042328042328,\n",
       " 0.6851851851851852,\n",
       " 0.6772486772486772]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras import applications\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, merge, Input, concatenate\n",
    "from keras.models import Model\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Flatten, LSTM, GRU, SpatialDropout1D\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers import Input, LSTM, Embedding, BatchNormalization, MaxPooling1D, concatenate, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "112/112 [==============================] - 4s 37ms/step - loss: 0.6932 - accuracy: 0.5282 - val_loss: 0.6923 - val_accuracy: 0.5201\n",
      "Epoch 2/10\n",
      "112/112 [==============================] - 4s 32ms/step - loss: 0.6935 - accuracy: 0.5327 - val_loss: 0.6917 - val_accuracy: 0.5335\n",
      "Epoch 3/10\n",
      "112/112 [==============================] - 4s 34ms/step - loss: 0.6927 - accuracy: 0.5291 - val_loss: 0.6922 - val_accuracy: 0.5268\n",
      "Epoch 4/10\n",
      "112/112 [==============================] - 4s 34ms/step - loss: 0.6930 - accuracy: 0.5282 - val_loss: 0.6902 - val_accuracy: 0.5362\n",
      "Epoch 5/10\n",
      "112/112 [==============================] - 4s 32ms/step - loss: 0.6913 - accuracy: 0.5318 - val_loss: 0.6901 - val_accuracy: 0.5362\n",
      "Epoch 6/10\n",
      "112/112 [==============================] - 4s 34ms/step - loss: 0.6918 - accuracy: 0.5300 - val_loss: 0.6899 - val_accuracy: 0.5362\n",
      "Epoch 7/10\n",
      "112/112 [==============================] - 4s 33ms/step - loss: 0.6989 - accuracy: 0.5121 - val_loss: 0.6911 - val_accuracy: 0.5349\n",
      "Epoch 8/10\n",
      "112/112 [==============================] - 4s 32ms/step - loss: 0.6919 - accuracy: 0.5237 - val_loss: 0.6907 - val_accuracy: 0.5389\n",
      "Epoch 9/10\n",
      "112/112 [==============================] - 4s 33ms/step - loss: 0.6921 - accuracy: 0.5363 - val_loss: 0.6915 - val_accuracy: 0.5228\n",
      "Epoch 10/10\n",
      "112/112 [==============================] - 4s 32ms/step - loss: 0.6901 - accuracy: 0.5380 - val_loss: 0.6908 - val_accuracy: 0.5375\n",
      "38/38 - 0s - loss: 0.6962 - accuracy: 0.5079\n",
      "Accuracy: 0.51\n",
      "Precision: 0.508\n",
      "Recall: 0.990\n",
      "F1-Score: 0.671\n",
      "max number of features used : 500\n",
      "ngram_range (1,1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.01      0.02       186\n",
      "           1       0.51      0.99      0.67       192\n",
      "\n",
      "    accuracy                           0.51       378\n",
      "   macro avg       0.50      0.50      0.35       378\n",
      "weighted avg       0.50      0.51      0.35       378\n",
      "\n",
      "--- 49.87086510658264 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Epoch 1/10\n",
      "112/112 [==============================] - 4s 38ms/step - loss: 0.6946 - accuracy: 0.5210 - val_loss: 0.6906 - val_accuracy: 0.5362\n",
      "Epoch 2/10\n",
      "112/112 [==============================] - 4s 32ms/step - loss: 0.6923 - accuracy: 0.5264 - val_loss: 0.6922 - val_accuracy: 0.5375\n",
      "Epoch 3/10\n",
      "112/112 [==============================] - 4s 32ms/step - loss: 0.6923 - accuracy: 0.5282 - val_loss: 0.6903 - val_accuracy: 0.5362\n",
      "Epoch 4/10\n",
      "112/112 [==============================] - 4s 32ms/step - loss: 0.6923 - accuracy: 0.5336 - val_loss: 0.6902 - val_accuracy: 0.5375\n",
      "Epoch 5/10\n",
      "112/112 [==============================] - 4s 32ms/step - loss: 0.6916 - accuracy: 0.5336 - val_loss: 0.6904 - val_accuracy: 0.5375\n",
      "Epoch 6/10\n",
      "112/112 [==============================] - 4s 33ms/step - loss: 0.6907 - accuracy: 0.5300 - val_loss: 0.6911 - val_accuracy: 0.5375\n",
      "Epoch 7/10\n",
      "112/112 [==============================] - 4s 32ms/step - loss: 0.6926 - accuracy: 0.5327 - val_loss: 0.6901 - val_accuracy: 0.5375\n",
      "Epoch 8/10\n",
      "112/112 [==============================] - 4s 31ms/step - loss: 0.6909 - accuracy: 0.5336 - val_loss: 0.6902 - val_accuracy: 0.5389\n",
      "Epoch 9/10\n",
      "112/112 [==============================] - 4s 33ms/step - loss: 0.6914 - accuracy: 0.5228 - val_loss: 0.6904 - val_accuracy: 0.5389\n",
      "Epoch 10/10\n",
      "112/112 [==============================] - 4s 34ms/step - loss: 0.6914 - accuracy: 0.5345 - val_loss: 0.6900 - val_accuracy: 0.5416\n",
      "38/38 - 0s - loss: 0.6923 - accuracy: 0.5185\n",
      "Accuracy: 0.52\n",
      "Precision: 0.514\n",
      "Recall: 0.990\n",
      "F1-Score: 0.676\n",
      "max number of features used : 500\n",
      "ngram_range (2,2)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.03      0.06       186\n",
      "           1       0.51      0.99      0.68       192\n",
      "\n",
      "    accuracy                           0.52       378\n",
      "   macro avg       0.63      0.51      0.37       378\n",
      "weighted avg       0.63      0.52      0.37       378\n",
      "\n",
      "--- 43.66956353187561 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Epoch 1/10\n",
      "112/112 [==============================] - 4s 36ms/step - loss: 0.6955 - accuracy: 0.5228 - val_loss: 0.6911 - val_accuracy: 0.5375\n",
      "Epoch 2/10\n",
      "112/112 [==============================] - 4s 32ms/step - loss: 0.6915 - accuracy: 0.5309 - val_loss: 0.6907 - val_accuracy: 0.5375\n",
      "Epoch 3/10\n",
      "112/112 [==============================] - 4s 31ms/step - loss: 0.6919 - accuracy: 0.5201 - val_loss: 0.6910 - val_accuracy: 0.5295\n",
      "Epoch 4/10\n",
      "112/112 [==============================] - 4s 31ms/step - loss: 0.6915 - accuracy: 0.5336 - val_loss: 0.6913 - val_accuracy: 0.5308\n",
      "Epoch 5/10\n",
      "112/112 [==============================] - 4s 32ms/step - loss: 0.6915 - accuracy: 0.5363 - val_loss: 0.6896 - val_accuracy: 0.5282\n",
      "Epoch 6/10\n",
      "112/112 [==============================] - 4s 33ms/step - loss: 0.6932 - accuracy: 0.5380 - val_loss: 0.6896 - val_accuracy: 0.5335\n",
      "Epoch 7/10\n",
      "112/112 [==============================] - 3s 31ms/step - loss: 0.6930 - accuracy: 0.5228 - val_loss: 0.6896 - val_accuracy: 0.5429\n",
      "Epoch 8/10\n",
      "112/112 [==============================] - 4s 32ms/step - loss: 0.6908 - accuracy: 0.5380 - val_loss: 0.6900 - val_accuracy: 0.5308\n",
      "Epoch 9/10\n",
      "112/112 [==============================] - 4s 32ms/step - loss: 0.6910 - accuracy: 0.5363 - val_loss: 0.6898 - val_accuracy: 0.5389\n",
      "Epoch 10/10\n",
      "112/112 [==============================] - 4s 31ms/step - loss: 0.6914 - accuracy: 0.5398 - val_loss: 0.6898 - val_accuracy: 0.5322\n",
      "38/38 - 0s - loss: 0.6931 - accuracy: 0.5238\n",
      "Accuracy: 0.52\n",
      "Precision: 0.518\n",
      "Recall: 0.917\n",
      "F1-Score: 0.662\n",
      "max number of features used : 500\n",
      "ngram_range (3,3)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.12      0.20       186\n",
      "           1       0.52      0.92      0.66       192\n",
      "\n",
      "    accuracy                           0.52       378\n",
      "   macro avg       0.55      0.52      0.43       378\n",
      "weighted avg       0.55      0.52      0.43       378\n",
      "\n",
      "--- 45.19626450538635 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Epoch 1/10\n",
      "112/112 [==============================] - 4s 35ms/step - loss: 0.6958 - accuracy: 0.5273 - val_loss: 0.6905 - val_accuracy: 0.5362\n",
      "Epoch 2/10\n",
      "112/112 [==============================] - 4s 36ms/step - loss: 0.6919 - accuracy: 0.5318 - val_loss: 0.6916 - val_accuracy: 0.5322\n",
      "Epoch 3/10\n",
      "112/112 [==============================] - 4s 34ms/step - loss: 0.6915 - accuracy: 0.5157 - val_loss: 0.6920 - val_accuracy: 0.5362\n",
      "Epoch 4/10\n",
      "112/112 [==============================] - 4s 32ms/step - loss: 0.6919 - accuracy: 0.5327 - val_loss: 0.6906 - val_accuracy: 0.5362\n",
      "Epoch 5/10\n",
      "112/112 [==============================] - 4s 33ms/step - loss: 0.6922 - accuracy: 0.5300 - val_loss: 0.6905 - val_accuracy: 0.5362\n",
      "Epoch 6/10\n",
      "112/112 [==============================] - 4s 32ms/step - loss: 0.6922 - accuracy: 0.5327 - val_loss: 0.6905 - val_accuracy: 0.5362\n",
      "Epoch 7/10\n",
      "112/112 [==============================] - 4s 31ms/step - loss: 0.6920 - accuracy: 0.5327 - val_loss: 0.6905 - val_accuracy: 0.5362\n",
      "Epoch 8/10\n",
      "112/112 [==============================] - 4s 33ms/step - loss: 0.6915 - accuracy: 0.5327 - val_loss: 0.6905 - val_accuracy: 0.5362\n",
      "Epoch 9/10\n",
      "112/112 [==============================] - 3s 31ms/step - loss: 0.6916 - accuracy: 0.5327 - val_loss: 0.6905 - val_accuracy: 0.5362\n",
      "Epoch 10/10\n",
      "112/112 [==============================] - 4s 31ms/step - loss: 0.6916 - accuracy: 0.5300 - val_loss: 0.6906 - val_accuracy: 0.5362\n",
      "38/38 - 0s - loss: 0.6937 - accuracy: 0.5079\n",
      "Accuracy: 0.51\n",
      "Precision: 0.508\n",
      "Recall: 1.000\n",
      "F1-Score: 0.674\n",
      "max number of features used : 500\n",
      "ngram_range (4,4)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       186\n",
      "           1       0.51      1.00      0.67       192\n",
      "\n",
      "    accuracy                           0.51       378\n",
      "   macro avg       0.25      0.50      0.34       378\n",
      "weighted avg       0.26      0.51      0.34       378\n",
      "\n",
      "--- 45.1302764415741 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "112/112 [==============================] - 4s 36ms/step - loss: 0.6932 - accuracy: 0.5192 - val_loss: 0.6908 - val_accuracy: 0.5362\n",
      "Epoch 2/10\n",
      "112/112 [==============================] - 4s 33ms/step - loss: 0.6933 - accuracy: 0.5166 - val_loss: 0.6908 - val_accuracy: 0.5362\n",
      "Epoch 3/10\n",
      "112/112 [==============================] - 4s 31ms/step - loss: 0.6921 - accuracy: 0.5237 - val_loss: 0.6917 - val_accuracy: 0.5349\n",
      "Epoch 4/10\n",
      "112/112 [==============================] - 4s 31ms/step - loss: 0.6923 - accuracy: 0.5336 - val_loss: 0.6908 - val_accuracy: 0.5362\n",
      "Epoch 5/10\n",
      "112/112 [==============================] - 4s 32ms/step - loss: 0.6918 - accuracy: 0.5327 - val_loss: 0.6911 - val_accuracy: 0.5349\n",
      "Epoch 6/10\n",
      "112/112 [==============================] - 4s 34ms/step - loss: 0.6916 - accuracy: 0.5336 - val_loss: 0.6906 - val_accuracy: 0.5362\n",
      "Epoch 7/10\n",
      "112/112 [==============================] - 4s 32ms/step - loss: 0.6914 - accuracy: 0.5264 - val_loss: 0.6923 - val_accuracy: 0.5362\n",
      "Epoch 8/10\n",
      "112/112 [==============================] - 4s 34ms/step - loss: 0.6931 - accuracy: 0.5327 - val_loss: 0.6914 - val_accuracy: 0.5349\n",
      "Epoch 9/10\n",
      "112/112 [==============================] - 4s 33ms/step - loss: 0.6914 - accuracy: 0.5318 - val_loss: 0.6910 - val_accuracy: 0.5362\n",
      "Epoch 10/10\n",
      "112/112 [==============================] - 4s 32ms/step - loss: 0.6914 - accuracy: 0.5336 - val_loss: 0.6907 - val_accuracy: 0.5349\n",
      "38/38 - 0s - loss: 0.6945 - accuracy: 0.5079\n",
      "Accuracy: 0.51\n",
      "Precision: 0.508\n",
      "Recall: 0.995\n",
      "F1-Score: 0.673\n",
      "max number of features used : 500\n",
      "ngram_range (5,5)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.01      0.01       186\n",
      "           1       0.51      0.99      0.67       192\n",
      "\n",
      "    accuracy                           0.51       378\n",
      "   macro avg       0.50      0.50      0.34       378\n",
      "weighted avg       0.50      0.51      0.35       378\n",
      "\n",
      "--- 46.17913341522217 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Epoch 1/10\n",
      "112/112 [==============================] - 7s 62ms/step - loss: 0.6945 - accuracy: 0.5103 - val_loss: 0.6907 - val_accuracy: 0.5362\n",
      "Epoch 2/10\n",
      "112/112 [==============================] - 6s 57ms/step - loss: 0.6928 - accuracy: 0.5228 - val_loss: 0.6912 - val_accuracy: 0.5362\n",
      "Epoch 3/10\n",
      "112/112 [==============================] - 6s 57ms/step - loss: 0.6927 - accuracy: 0.5264 - val_loss: 0.6907 - val_accuracy: 0.5362\n",
      "Epoch 4/10\n",
      "112/112 [==============================] - 6s 58ms/step - loss: 0.6922 - accuracy: 0.5327 - val_loss: 0.6908 - val_accuracy: 0.5362\n",
      "Epoch 5/10\n",
      "112/112 [==============================] - 6s 57ms/step - loss: 0.6918 - accuracy: 0.5264 - val_loss: 0.6921 - val_accuracy: 0.5362\n",
      "Epoch 6/10\n",
      "112/112 [==============================] - 6s 57ms/step - loss: 0.6919 - accuracy: 0.5318 - val_loss: 0.6909 - val_accuracy: 0.5362\n",
      "Epoch 7/10\n",
      "112/112 [==============================] - 7s 59ms/step - loss: 0.6916 - accuracy: 0.5327 - val_loss: 0.6910 - val_accuracy: 0.5362\n",
      "Epoch 8/10\n",
      "112/112 [==============================] - 7s 58ms/step - loss: 0.6914 - accuracy: 0.5327 - val_loss: 0.6912 - val_accuracy: 0.5362\n",
      "Epoch 9/10\n",
      "112/112 [==============================] - 7s 59ms/step - loss: 0.6905 - accuracy: 0.5327 - val_loss: 0.6914 - val_accuracy: 0.5362\n",
      "Epoch 10/10\n",
      "112/112 [==============================] - 6s 57ms/step - loss: 0.6918 - accuracy: 0.5327 - val_loss: 0.6910 - val_accuracy: 0.5362\n",
      "38/38 - 1s - loss: 0.6935 - accuracy: 0.5079\n",
      "Accuracy: 0.51\n",
      "Precision: 0.508\n",
      "Recall: 1.000\n",
      "F1-Score: 0.674\n",
      "max number of features used : 1000\n",
      "ngram_range (1,1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       186\n",
      "           1       0.51      1.00      0.67       192\n",
      "\n",
      "    accuracy                           0.51       378\n",
      "   macro avg       0.25      0.50      0.34       378\n",
      "weighted avg       0.26      0.51      0.34       378\n",
      "\n",
      "--- 70.4816472530365 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Epoch 1/10\n",
      "112/112 [==============================] - 7s 61ms/step - loss: 0.6937 - accuracy: 0.5389 - val_loss: 0.6910 - val_accuracy: 0.5362\n",
      "Epoch 2/10\n",
      "112/112 [==============================] - 6s 57ms/step - loss: 0.6921 - accuracy: 0.5318 - val_loss: 0.6932 - val_accuracy: 0.5402\n",
      "Epoch 3/10\n",
      "112/112 [==============================] - 6s 58ms/step - loss: 0.6907 - accuracy: 0.5354 - val_loss: 0.6948 - val_accuracy: 0.5402\n",
      "Epoch 4/10\n",
      "112/112 [==============================] - 6s 57ms/step - loss: 0.6904 - accuracy: 0.5201 - val_loss: 0.6955 - val_accuracy: 0.5429\n",
      "Epoch 5/10\n",
      "112/112 [==============================] - 7s 59ms/step - loss: 0.6882 - accuracy: 0.5264 - val_loss: 0.7031 - val_accuracy: 0.4598\n",
      "Epoch 6/10\n",
      "112/112 [==============================] - 7s 58ms/step - loss: 0.6893 - accuracy: 0.5416 - val_loss: 0.6965 - val_accuracy: 0.5335\n",
      "Epoch 7/10\n",
      "112/112 [==============================] - 6s 57ms/step - loss: 0.6885 - accuracy: 0.5300 - val_loss: 0.6973 - val_accuracy: 0.5362\n",
      "Epoch 8/10\n",
      "112/112 [==============================] - 6s 58ms/step - loss: 0.6889 - accuracy: 0.5300 - val_loss: 0.6964 - val_accuracy: 0.5268\n",
      "Epoch 9/10\n",
      "112/112 [==============================] - 6s 57ms/step - loss: 0.6893 - accuracy: 0.5201 - val_loss: 0.6977 - val_accuracy: 0.5295\n",
      "Epoch 10/10\n",
      "112/112 [==============================] - 6s 56ms/step - loss: 0.6883 - accuracy: 0.5318 - val_loss: 0.6976 - val_accuracy: 0.5335\n",
      "38/38 - 1s - loss: 0.6916 - accuracy: 0.5079\n",
      "Accuracy: 0.51\n",
      "Precision: 0.508\n",
      "Recall: 0.948\n",
      "F1-Score: 0.662\n",
      "max number of features used : 1000\n",
      "ngram_range (2,2)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.05      0.10       186\n",
      "           1       0.51      0.95      0.66       192\n",
      "\n",
      "    accuracy                           0.51       378\n",
      "   macro avg       0.50      0.50      0.38       378\n",
      "weighted avg       0.50      0.51      0.38       378\n",
      "\n",
      "--- 71.8125171661377 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Epoch 1/10\n",
      "112/112 [==============================] - 8s 67ms/step - loss: 0.6940 - accuracy: 0.5192 - val_loss: 0.6906 - val_accuracy: 0.5362\n",
      "Epoch 2/10\n",
      "112/112 [==============================] - 6s 57ms/step - loss: 0.6927 - accuracy: 0.5237 - val_loss: 0.6915 - val_accuracy: 0.5295\n",
      "Epoch 3/10\n",
      "112/112 [==============================] - 7s 59ms/step - loss: 0.6919 - accuracy: 0.5345 - val_loss: 0.6906 - val_accuracy: 0.5375\n",
      "Epoch 4/10\n",
      "112/112 [==============================] - 6s 57ms/step - loss: 0.6920 - accuracy: 0.5291 - val_loss: 0.6907 - val_accuracy: 0.5375\n",
      "Epoch 5/10\n",
      "112/112 [==============================] - 6s 57ms/step - loss: 0.6923 - accuracy: 0.5327 - val_loss: 0.6911 - val_accuracy: 0.5295\n",
      "Epoch 6/10\n",
      "112/112 [==============================] - 6s 57ms/step - loss: 0.6915 - accuracy: 0.5336 - val_loss: 0.6907 - val_accuracy: 0.5295\n",
      "Epoch 7/10\n",
      "112/112 [==============================] - 6s 58ms/step - loss: 0.6916 - accuracy: 0.5354 - val_loss: 0.6912 - val_accuracy: 0.5295\n",
      "Epoch 8/10\n",
      "112/112 [==============================] - 6s 57ms/step - loss: 0.6913 - accuracy: 0.5372 - val_loss: 0.6910 - val_accuracy: 0.5295\n",
      "Epoch 9/10\n",
      "112/112 [==============================] - 6s 56ms/step - loss: 0.6910 - accuracy: 0.5363 - val_loss: 0.6909 - val_accuracy: 0.5349\n",
      "Epoch 10/10\n",
      "112/112 [==============================] - 6s 57ms/step - loss: 0.6908 - accuracy: 0.5345 - val_loss: 0.6912 - val_accuracy: 0.5349\n",
      "38/38 - 1s - loss: 0.6919 - accuracy: 0.5265\n",
      "Accuracy: 0.53\n",
      "Precision: 0.519\n",
      "Recall: 0.932\n",
      "F1-Score: 0.667\n",
      "max number of features used : 1000\n",
      "ngram_range (3,3)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.11      0.18       186\n",
      "           1       0.52      0.93      0.67       192\n",
      "\n",
      "    accuracy                           0.53       378\n",
      "   macro avg       0.56      0.52      0.42       378\n",
      "weighted avg       0.56      0.53      0.43       378\n",
      "\n",
      "--- 73.42179274559021 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "112/112 [==============================] - 7s 62ms/step - loss: 0.6943 - accuracy: 0.5300 - val_loss: 0.6916 - val_accuracy: 0.5362\n",
      "Epoch 2/10\n",
      "112/112 [==============================] - 6s 57ms/step - loss: 0.6930 - accuracy: 0.5184 - val_loss: 0.6914 - val_accuracy: 0.5335\n",
      "Epoch 3/10\n",
      "112/112 [==============================] - 6s 57ms/step - loss: 0.6915 - accuracy: 0.5336 - val_loss: 0.6916 - val_accuracy: 0.5362\n",
      "Epoch 4/10\n",
      "112/112 [==============================] - 6s 56ms/step - loss: 0.6938 - accuracy: 0.5300 - val_loss: 0.6906 - val_accuracy: 0.5362\n",
      "Epoch 5/10\n",
      "112/112 [==============================] - 6s 57ms/step - loss: 0.6916 - accuracy: 0.5327 - val_loss: 0.6911 - val_accuracy: 0.5335\n",
      "Epoch 6/10\n",
      "112/112 [==============================] - 6s 58ms/step - loss: 0.6916 - accuracy: 0.5336 - val_loss: 0.6909 - val_accuracy: 0.5362\n",
      "Epoch 7/10\n",
      "112/112 [==============================] - 6s 57ms/step - loss: 0.6919 - accuracy: 0.5345 - val_loss: 0.6908 - val_accuracy: 0.5335\n",
      "Epoch 8/10\n",
      "112/112 [==============================] - 6s 57ms/step - loss: 0.6913 - accuracy: 0.5345 - val_loss: 0.6911 - val_accuracy: 0.5335\n",
      "Epoch 9/10\n",
      "112/112 [==============================] - 6s 56ms/step - loss: 0.6913 - accuracy: 0.5345 - val_loss: 0.6909 - val_accuracy: 0.5335\n",
      "Epoch 10/10\n",
      "112/112 [==============================] - 7s 59ms/step - loss: 0.6910 - accuracy: 0.5345 - val_loss: 0.6910 - val_accuracy: 0.5335\n",
      "38/38 - 1s - loss: 0.6950 - accuracy: 0.5053\n",
      "Accuracy: 0.51\n",
      "Precision: 0.507\n",
      "Recall: 0.995\n",
      "F1-Score: 0.671\n",
      "max number of features used : 1000\n",
      "ngram_range (4,4)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       186\n",
      "           1       0.51      0.99      0.67       192\n",
      "\n",
      "    accuracy                           0.51       378\n",
      "   macro avg       0.25      0.50      0.34       378\n",
      "weighted avg       0.26      0.51      0.34       378\n",
      "\n",
      "--- 74.07399916648865 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Epoch 1/10\n",
      "112/112 [==============================] - 7s 60ms/step - loss: 0.6947 - accuracy: 0.5273 - val_loss: 0.6910 - val_accuracy: 0.5362\n",
      "Epoch 2/10\n",
      "112/112 [==============================] - 6s 57ms/step - loss: 0.6931 - accuracy: 0.5327 - val_loss: 0.6913 - val_accuracy: 0.5362\n",
      "Epoch 3/10\n",
      "112/112 [==============================] - 6s 56ms/step - loss: 0.6920 - accuracy: 0.5210 - val_loss: 0.6911 - val_accuracy: 0.5362\n",
      "Epoch 4/10\n",
      "112/112 [==============================] - 7s 59ms/step - loss: 0.6923 - accuracy: 0.5327 - val_loss: 0.6906 - val_accuracy: 0.5362\n",
      "Epoch 5/10\n",
      "112/112 [==============================] - 6s 57ms/step - loss: 0.6921 - accuracy: 0.5327 - val_loss: 0.6907 - val_accuracy: 0.5362\n",
      "Epoch 6/10\n",
      "112/112 [==============================] - 6s 56ms/step - loss: 0.6920 - accuracy: 0.5327 - val_loss: 0.6907 - val_accuracy: 0.5362\n",
      "Epoch 7/10\n",
      "112/112 [==============================] - 6s 57ms/step - loss: 0.6915 - accuracy: 0.5327 - val_loss: 0.6908 - val_accuracy: 0.5349\n",
      "Epoch 8/10\n",
      "112/112 [==============================] - 7s 60ms/step - loss: 0.6917 - accuracy: 0.5336 - val_loss: 0.6907 - val_accuracy: 0.5349\n",
      "Epoch 9/10\n",
      "112/112 [==============================] - 7s 59ms/step - loss: 0.6918 - accuracy: 0.5327 - val_loss: 0.6907 - val_accuracy: 0.5349\n",
      "Epoch 10/10\n",
      "112/112 [==============================] - 6s 56ms/step - loss: 0.6913 - accuracy: 0.5336 - val_loss: 0.6907 - val_accuracy: 0.5349\n",
      "38/38 - 1s - loss: 0.6946 - accuracy: 0.5079\n",
      "Accuracy: 0.51\n",
      "Precision: 0.508\n",
      "Recall: 0.995\n",
      "F1-Score: 0.673\n",
      "max number of features used : 1000\n",
      "ngram_range (5,5)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.01      0.01       186\n",
      "           1       0.51      0.99      0.67       192\n",
      "\n",
      "    accuracy                           0.51       378\n",
      "   macro avg       0.50      0.50      0.34       378\n",
      "weighted avg       0.50      0.51      0.35       378\n",
      "\n",
      "--- 73.84564518928528 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Epoch 1/10\n",
      "112/112 [==============================] - 12s 112ms/step - loss: 0.6945 - accuracy: 0.5192 - val_loss: 0.6916 - val_accuracy: 0.5362\n",
      "Epoch 2/10\n",
      "112/112 [==============================] - 12s 108ms/step - loss: 0.6922 - accuracy: 0.5318 - val_loss: 0.6912 - val_accuracy: 0.5362\n",
      "Epoch 3/10\n",
      "112/112 [==============================] - 12s 110ms/step - loss: 0.6908 - accuracy: 0.5363 - val_loss: 0.6961 - val_accuracy: 0.4665\n",
      "Epoch 4/10\n",
      "112/112 [==============================] - 12s 110ms/step - loss: 0.6934 - accuracy: 0.5201 - val_loss: 0.6910 - val_accuracy: 0.5362\n",
      "Epoch 5/10\n",
      "112/112 [==============================] - 12s 110ms/step - loss: 0.6918 - accuracy: 0.5282 - val_loss: 0.6918 - val_accuracy: 0.5362\n",
      "Epoch 6/10\n",
      "112/112 [==============================] - 12s 108ms/step - loss: 0.6922 - accuracy: 0.5291 - val_loss: 0.6910 - val_accuracy: 0.5362\n",
      "Epoch 7/10\n",
      "112/112 [==============================] - 12s 109ms/step - loss: 0.6915 - accuracy: 0.5291 - val_loss: 0.6910 - val_accuracy: 0.5362\n",
      "Epoch 8/10\n",
      "112/112 [==============================] - 12s 110ms/step - loss: 0.6909 - accuracy: 0.5327 - val_loss: 0.6913 - val_accuracy: 0.5362\n",
      "Epoch 9/10\n",
      "112/112 [==============================] - 12s 109ms/step - loss: 0.6904 - accuracy: 0.5345 - val_loss: 0.6926 - val_accuracy: 0.5228\n",
      "Epoch 10/10\n",
      "112/112 [==============================] - 12s 109ms/step - loss: 0.6915 - accuracy: 0.5184 - val_loss: 0.6920 - val_accuracy: 0.5322\n",
      "38/38 - 1s - loss: 0.6953 - accuracy: 0.5106\n",
      "Accuracy: 0.51\n",
      "Precision: 0.510\n",
      "Recall: 0.969\n",
      "F1-Score: 0.668\n",
      "max number of features used : 2000\n",
      "ngram_range (1,1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.04      0.07       186\n",
      "           1       0.51      0.97      0.67       192\n",
      "\n",
      "    accuracy                           0.51       378\n",
      "   macro avg       0.52      0.50      0.37       378\n",
      "weighted avg       0.52      0.51      0.37       378\n",
      "\n",
      "--- 129.87803435325623 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Epoch 1/10\n",
      "112/112 [==============================] - 13s 112ms/step - loss: 0.6947 - accuracy: 0.5058 - val_loss: 0.6926 - val_accuracy: 0.5362\n",
      "Epoch 2/10\n",
      "112/112 [==============================] - 12s 110ms/step - loss: 0.6909 - accuracy: 0.5327 - val_loss: 0.6923 - val_accuracy: 0.5349\n",
      "Epoch 3/10\n",
      "112/112 [==============================] - 12s 112ms/step - loss: 0.6912 - accuracy: 0.5237 - val_loss: 0.6984 - val_accuracy: 0.4491\n",
      "Epoch 4/10\n",
      "112/112 [==============================] - 12s 108ms/step - loss: 0.6917 - accuracy: 0.5184 - val_loss: 0.6961 - val_accuracy: 0.5308\n",
      "Epoch 5/10\n",
      "112/112 [==============================] - 12s 109ms/step - loss: 0.6892 - accuracy: 0.5389 - val_loss: 0.6966 - val_accuracy: 0.5349\n",
      "Epoch 6/10\n",
      "112/112 [==============================] - 12s 108ms/step - loss: 0.6884 - accuracy: 0.5336 - val_loss: 0.6972 - val_accuracy: 0.5322\n",
      "Epoch 7/10\n",
      "112/112 [==============================] - 12s 111ms/step - loss: 0.6895 - accuracy: 0.5228 - val_loss: 0.6983 - val_accuracy: 0.5349\n",
      "Epoch 8/10\n",
      "112/112 [==============================] - 12s 109ms/step - loss: 0.6890 - accuracy: 0.5318 - val_loss: 0.6971 - val_accuracy: 0.5335\n",
      "Epoch 9/10\n",
      "112/112 [==============================] - 12s 108ms/step - loss: 0.6891 - accuracy: 0.5416 - val_loss: 0.6961 - val_accuracy: 0.5335\n",
      "Epoch 10/10\n",
      "112/112 [==============================] - 12s 108ms/step - loss: 0.6899 - accuracy: 0.5309 - val_loss: 0.6956 - val_accuracy: 0.5349\n",
      "38/38 - 1s - loss: 0.6930 - accuracy: 0.5079\n",
      "Accuracy: 0.51\n",
      "Precision: 0.508\n",
      "Recall: 1.000\n",
      "F1-Score: 0.674\n",
      "max number of features used : 2000\n",
      "ngram_range (2,2)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       186\n",
      "           1       0.51      1.00      0.67       192\n",
      "\n",
      "    accuracy                           0.51       378\n",
      "   macro avg       0.25      0.50      0.34       378\n",
      "weighted avg       0.26      0.51      0.34       378\n",
      "\n",
      "--- 131.02540946006775 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "112/112 [==============================] - 13s 113ms/step - loss: 0.6950 - accuracy: 0.5049 - val_loss: 0.6911 - val_accuracy: 0.5362\n",
      "Epoch 2/10\n",
      "112/112 [==============================] - 12s 110ms/step - loss: 0.6926 - accuracy: 0.5282 - val_loss: 0.6909 - val_accuracy: 0.5362\n",
      "Epoch 3/10\n",
      "112/112 [==============================] - 12s 108ms/step - loss: 0.6927 - accuracy: 0.5318 - val_loss: 0.6912 - val_accuracy: 0.5362\n",
      "Epoch 4/10\n",
      "112/112 [==============================] - 12s 109ms/step - loss: 0.6921 - accuracy: 0.5318 - val_loss: 0.6910 - val_accuracy: 0.5362\n",
      "Epoch 5/10\n",
      "112/112 [==============================] - 12s 108ms/step - loss: 0.6914 - accuracy: 0.5327 - val_loss: 0.6910 - val_accuracy: 0.5349\n",
      "Epoch 6/10\n",
      "112/112 [==============================] - 12s 109ms/step - loss: 0.6915 - accuracy: 0.5327 - val_loss: 0.6907 - val_accuracy: 0.5349\n",
      "Epoch 7/10\n",
      "112/112 [==============================] - 12s 110ms/step - loss: 0.6927 - accuracy: 0.5309 - val_loss: 0.6911 - val_accuracy: 0.5349\n",
      "Epoch 8/10\n",
      "112/112 [==============================] - 12s 107ms/step - loss: 0.6918 - accuracy: 0.5309 - val_loss: 0.6909 - val_accuracy: 0.5349\n",
      "Epoch 9/10\n",
      "112/112 [==============================] - 12s 108ms/step - loss: 0.6914 - accuracy: 0.5336 - val_loss: 0.6907 - val_accuracy: 0.5349\n",
      "Epoch 10/10\n",
      "112/112 [==============================] - 12s 108ms/step - loss: 0.6914 - accuracy: 0.5327 - val_loss: 0.6907 - val_accuracy: 0.5349\n",
      "38/38 - 1s - loss: 0.6948 - accuracy: 0.5026\n",
      "Accuracy: 0.50\n",
      "Precision: 0.505\n",
      "Recall: 0.990\n",
      "F1-Score: 0.669\n",
      "max number of features used : 2000\n",
      "ngram_range (3,3)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       186\n",
      "           1       0.51      0.99      0.67       192\n",
      "\n",
      "    accuracy                           0.50       378\n",
      "   macro avg       0.25      0.49      0.33       378\n",
      "weighted avg       0.26      0.50      0.34       378\n",
      "\n",
      "--- 132.0510220527649 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Epoch 1/10\n",
      "112/112 [==============================] - 13s 113ms/step - loss: 0.6931 - accuracy: 0.5228 - val_loss: 0.6907 - val_accuracy: 0.5362\n",
      "Epoch 2/10\n",
      "112/112 [==============================] - 12s 109ms/step - loss: 0.6921 - accuracy: 0.5184 - val_loss: 0.6918 - val_accuracy: 0.5322\n",
      "Epoch 3/10\n",
      "112/112 [==============================] - 12s 108ms/step - loss: 0.6930 - accuracy: 0.5219 - val_loss: 0.6908 - val_accuracy: 0.5335\n",
      "Epoch 4/10\n",
      "112/112 [==============================] - 12s 109ms/step - loss: 0.6907 - accuracy: 0.5363 - val_loss: 0.6917 - val_accuracy: 0.5335\n",
      "Epoch 5/10\n",
      "112/112 [==============================] - 12s 110ms/step - loss: 0.6917 - accuracy: 0.5345 - val_loss: 0.6911 - val_accuracy: 0.5335\n",
      "Epoch 6/10\n",
      "112/112 [==============================] - 12s 109ms/step - loss: 0.6914 - accuracy: 0.5345 - val_loss: 0.6911 - val_accuracy: 0.5335\n",
      "Epoch 7/10\n",
      "112/112 [==============================] - 12s 109ms/step - loss: 0.6914 - accuracy: 0.5345 - val_loss: 0.6915 - val_accuracy: 0.5335\n",
      "Epoch 8/10\n",
      "112/112 [==============================] - 12s 108ms/step - loss: 0.6920 - accuracy: 0.5345 - val_loss: 0.6915 - val_accuracy: 0.5335\n",
      "Epoch 9/10\n",
      "112/112 [==============================] - 12s 108ms/step - loss: 0.6909 - accuracy: 0.5345 - val_loss: 0.6928 - val_accuracy: 0.5335\n",
      "Epoch 10/10\n",
      "112/112 [==============================] - 12s 110ms/step - loss: 0.6913 - accuracy: 0.5345 - val_loss: 0.6926 - val_accuracy: 0.5335\n",
      "38/38 - 1s - loss: 0.6955 - accuracy: 0.5053\n",
      "Accuracy: 0.51\n",
      "Precision: 0.507\n",
      "Recall: 0.995\n",
      "F1-Score: 0.671\n",
      "max number of features used : 2000\n",
      "ngram_range (4,4)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       186\n",
      "           1       0.51      0.99      0.67       192\n",
      "\n",
      "    accuracy                           0.51       378\n",
      "   macro avg       0.25      0.50      0.34       378\n",
      "weighted avg       0.26      0.51      0.34       378\n",
      "\n",
      "--- 133.3382852077484 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Epoch 1/10\n",
      "112/112 [==============================] - 13s 112ms/step - loss: 0.6948 - accuracy: 0.5157 - val_loss: 0.6909 - val_accuracy: 0.5362\n",
      "Epoch 2/10\n",
      "112/112 [==============================] - 12s 109ms/step - loss: 0.6930 - accuracy: 0.5327 - val_loss: 0.6906 - val_accuracy: 0.5362\n",
      "Epoch 3/10\n",
      "112/112 [==============================] - 12s 107ms/step - loss: 0.6930 - accuracy: 0.5282 - val_loss: 0.6906 - val_accuracy: 0.5362\n",
      "Epoch 4/10\n",
      "112/112 [==============================] - 13s 117ms/step - loss: 0.6922 - accuracy: 0.5327 - val_loss: 0.6919 - val_accuracy: 0.5349\n",
      "Epoch 5/10\n",
      "112/112 [==============================] - 12s 109ms/step - loss: 0.6916 - accuracy: 0.5327 - val_loss: 0.6907 - val_accuracy: 0.5362\n",
      "Epoch 6/10\n",
      "112/112 [==============================] - 12s 108ms/step - loss: 0.6915 - accuracy: 0.5327 - val_loss: 0.6907 - val_accuracy: 0.5362\n",
      "Epoch 7/10\n",
      "112/112 [==============================] - 12s 109ms/step - loss: 0.6912 - accuracy: 0.5327 - val_loss: 0.6910 - val_accuracy: 0.5349\n",
      "Epoch 8/10\n",
      "112/112 [==============================] - 12s 108ms/step - loss: 0.6915 - accuracy: 0.5336 - val_loss: 0.6909 - val_accuracy: 0.5349\n",
      "Epoch 9/10\n",
      "112/112 [==============================] - 12s 110ms/step - loss: 0.6919 - accuracy: 0.5336 - val_loss: 0.6911 - val_accuracy: 0.5349\n",
      "Epoch 10/10\n",
      "112/112 [==============================] - 12s 108ms/step - loss: 0.6916 - accuracy: 0.5336 - val_loss: 0.6909 - val_accuracy: 0.5349\n",
      "38/38 - 1s - loss: 0.6938 - accuracy: 0.5079\n",
      "Accuracy: 0.51\n",
      "Precision: 0.508\n",
      "Recall: 0.995\n",
      "F1-Score: 0.673\n",
      "max number of features used : 2000\n",
      "ngram_range (5,5)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.01      0.01       186\n",
      "           1       0.51      0.99      0.67       192\n",
      "\n",
      "    accuracy                           0.51       378\n",
      "   macro avg       0.50      0.50      0.34       378\n",
      "weighted avg       0.50      0.51      0.35       378\n",
      "\n",
      "--- 133.37928771972656 seconds ---\n",
      "==========================================================================\n",
      "==========================================================================\n"
     ]
    }
   ],
   "source": [
    "max_features_num = [500,1000,2000]\n",
    "ngram = [1,2,3,4,5]\n",
    "accLSTM = []\n",
    "timeLSTM = []\n",
    "T = 0\n",
    "for i in max_features_num:\n",
    "    for j in ngram:\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        countvector=CountVectorizer(ngram_range=(j,j),max_features = i)\n",
    "        df_3 = countvector.fit_transform(headlines)\n",
    "        test_dataset = countvector.transform(test_transform)\n",
    "        \n",
    "        y_train = train['Label']\n",
    "        y_train = tf.keras.utils.to_categorical(y_train, 2)\n",
    "        y_test = test['Label']\n",
    "        y_test = tf.keras.utils.to_categorical(y_test, 2)\n",
    "        \n",
    "        #LSTM Model\n",
    "        mlp_model1=Input(shape=(df_3.shape[1],))\n",
    "        b1 = Embedding(input_dim=500, output_dim=128, input_length=100)(mlp_model1)\n",
    "        \n",
    "        df_3 = df_3.toarray()\n",
    "        test_dataset = test_dataset.toarray()\n",
    "\n",
    "        all_inp = b1\n",
    "        all_inp = SpatialDropout1D(0.2)(all_inp)\n",
    "        all_inp = LSTM(196)(all_inp)\n",
    "\n",
    "        p=Dense(2,activation='softmax')(all_inp)\n",
    "\n",
    "        \n",
    "        full_model = Model(inputs=[mlp_model1], outputs=p)\n",
    "        full_model.compile(loss='categorical_crossentropy',\n",
    "                           optimizer='adam',\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "        full_model.fit([df_3], y_train,\n",
    "                                 epochs=10, batch_size=10,\n",
    "                                 verbose=1, validation_split=0.4, shuffle = True)\n",
    "\n",
    "        _,acc = full_model.evaluate(test_dataset, y_test, verbose = 2, batch_size = 10)\n",
    "        y_pred = full_model.predict(test_dataset)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        y_test = np.argmax(y_test, axis=1)\n",
    "        \n",
    "        print(\"Accuracy: %.2f\" % (acc))\n",
    "        accLSTM.append(acc)\n",
    "        \n",
    "        precision = precision_score(y_test, y_pred, average='binary')\n",
    "        print('Precision: %.3f' % precision)\n",
    "        \n",
    "        recall = recall_score(y_test, y_pred, average='binary')\n",
    "        print('Recall: %.3f' % recall)\n",
    "        \n",
    "        score = f1_score(y_test, y_pred, average='binary')\n",
    "        print('F1-Score: %.3f' % score)\n",
    "        \n",
    "        print('max number of features used : {}'.format(i))\n",
    "        print('ngram_range ({},{})'.format(j,j))\n",
    "        print (classification_report(y_test, y_pred))\n",
    "       \n",
    "       \n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        T = time.time() - start_time\n",
    "        timeLSTM.append(T)\n",
    "        print('==========================================================================')\n",
    "        print('==========================================================================')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[49.87101340293884,\n",
       " 43.66960573196411,\n",
       " 45.196301221847534,\n",
       " 45.130311012268066,\n",
       " 46.17917013168335,\n",
       " 70.48169994354248,\n",
       " 71.81266260147095,\n",
       " 73.42182731628418,\n",
       " 74.07404971122742,\n",
       " 73.8456711769104,\n",
       " 129.87807869911194,\n",
       " 131.0254602432251,\n",
       " 132.0510561466217,\n",
       " 133.3383333683014,\n",
       " 133.37942957878113]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5079365372657776,\n",
       " 0.5185185074806213,\n",
       " 0.523809552192688,\n",
       " 0.5079365372657776,\n",
       " 0.5079365372657776,\n",
       " 0.5079365372657776,\n",
       " 0.5079365372657776,\n",
       " 0.5264550447463989,\n",
       " 0.5052909851074219,\n",
       " 0.5079365372657776,\n",
       " 0.5105820298194885,\n",
       " 0.5079365372657776,\n",
       " 0.5026454925537109,\n",
       " 0.5052909851074219,\n",
       " 0.5079365372657776]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [accLR,accSVM1,accSVM2,accRF,accXG,accGB,accLSTM]\n",
    "maxAcc = []\n",
    "minAcc = []\n",
    "for i in x:\n",
    "    k1 = max(i)\n",
    "    maxAcc.append(k1)\n",
    "    k2 = min(i)\n",
    "    minAcc.append(k2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8650793650793651,\n",
       " 0.8571428571428571,\n",
       " 0.8571428571428571,\n",
       " 0.8783068783068783,\n",
       " 0.8571428571428571,\n",
       " 0.7962962962962963,\n",
       " 0.5264550447463989]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxAcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6058201058201058,\n",
       " 0.5740740740740741,\n",
       " 0.6111111111111112,\n",
       " 0.6375661375661376,\n",
       " 0.5502645502645502,\n",
       " 0.5661375661375662,\n",
       " 0.5026454925537109]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minAcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7feedff111d0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "data = [maxAcc,minAcc]\n",
    "\n",
    "labels = ['Logistic Regression', 'SVM Gaussion', 'SVM Linear', 'Random Forest', 'XGBoost','Gradient Boosting','LSTM']\n",
    "X = np.arange(7)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.bar(X + 0.00, data[0], color = 'tab:blue', width = 0.25)\n",
    "ax.bar(X + 0.25, data[1], color = 'tab:orange', width = 0.25)\n",
    "\n",
    "\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Maximum and minimum accuracy of models')\n",
    "ax.set_xticks(X)\n",
    "degrees = 15\n",
    "plt.xticks(rotation=degrees)\n",
    "ax.set_xticklabels(labels)\n",
    "\n",
    "\n",
    "colors = {'Maximum Accuracy':'tab:blue', 'Minimum Accuracy':'tab:orange'}         \n",
    "labels = list(colors.keys())\n",
    "handles = [plt.Rectangle((0,0),1,1, color=colors[label]) for label in labels]\n",
    "ax.legend(handles, labels,bbox_to_anchor=(1.35, 1),loc = 'upper right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [timeLR,timeSVM1,timeSVM2,timeRF,timeXG,timeGB,timeLSTM]\n",
    "maxTime = []\n",
    "minTime = []\n",
    "for i in x:\n",
    "    k1 = max(i)\n",
    "    maxTime.append(k1)\n",
    "    k2 = min(i)\n",
    "    minTime.append(k2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.6019287109375,\n",
       " 11.335211753845215,\n",
       " 5.446606874465942,\n",
       " 8.310741662979126,\n",
       " 23.932716846466064,\n",
       " 6.593981504440308,\n",
       " 133.37942957878113]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.53847074508667,\n",
       " 4.7845189571380615,\n",
       " 2.4331042766571045,\n",
       " 5.920471906661987,\n",
       " 4.1674113273620605,\n",
       " 3.970543622970581,\n",
       " 43.66960573196411]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7feedf1f0a50>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [maxTime,minTime]\n",
    "\n",
    "labels = ['Logistic Regression', 'SVM Gaussion', 'SVM Linear', 'Random Forest', 'XGBoost','Gradient Boosting','LSTM']\n",
    "X = np.arange(7)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.bar(X + 0.00, data[0], color = 'tab:blue', width = 0.25)\n",
    "ax.bar(X + 0.25, data[1], color = 'tab:orange', width = 0.25)\n",
    "\n",
    "\n",
    "ax.set_ylabel('Execution Time')\n",
    "ax.set_title('Maximum and minimum execution time of models')\n",
    "ax.set_xticks(X)\n",
    "degrees = 15\n",
    "plt.xticks(rotation=degrees)\n",
    "ax.set_xticklabels(labels)\n",
    "\n",
    "\n",
    "colors = {'Maximum Execution Time':'tab:blue', 'Minimum Execution Time':'tab:orange'}         \n",
    "labels = list(colors.keys())\n",
    "handles = [plt.Rectangle((0,0),1,1, color=colors[label]) for label in labels]\n",
    "ax.legend(handles, labels,bbox_to_anchor=(1.42, 1),loc = 'upper right')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
